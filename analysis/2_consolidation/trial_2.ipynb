{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Hbol",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Trial 2\n",
    "something different, maybe. Hope this works! :')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../df.pkl\", \"rb\") as f:\n",
    "#     df = pickle.load(f)\n",
    "df = pd.read_excel('../data/merged_set.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Tools testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lEQa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from yake import KeywordExtractor\n",
    "# from keybert import KeyBERT\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### YAKE (Looks good, but too many keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_extractor = KeywordExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Used for testing purposes\n",
    "# text = df.loc[df[\"event_id\"] == \"case_1\"].iloc[0][\"content\"]\n",
    "# keywords = kw_extractor.extract_keywords(text)\n",
    "# keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kw_sum = 0\n",
    "# kw_sum = sum([keyword[1] for keyword in keywords])\n",
    "# kw_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Keybert (Useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Used for testing purposes\n",
    "# model = KeyBERT(model=\"all-mpnet-base-v2\")\n",
    "# keywords_kb = model.extract_keywords(text)\n",
    "# keywords_kb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iLit",
   "metadata": {},
   "source": [
    "### NER_xlm-roberta-large-finetuned-conll03-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "fair_ner_pipe = pipeline(\"token-classification\", model=\"FacebookAI/xlm-roberta-large-finetuned-conll03-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Used for testing purposes\n",
    "# raw_entities = fair_ner_pipe(text)\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_same_entities(_text, _raw_entities):\n",
    "    prev_segment = None\n",
    "    entities = []\n",
    "    for segment in _raw_entities:\n",
    "        segment['word'] = segment['word'].replace('▁', ' ')\n",
    "        original_entity_word = segment['word']\n",
    "        entity_word = segment['word'].rstrip()\n",
    "        segment['end'] = segment['end'] - (len(original_entity_word) - len(entity_word))\n",
    "        if segment['word'].isspace():\n",
    "            continue\n",
    "        appended_to_prev_segment = False\n",
    "        if prev_segment is not None and prev_segment['entity'] == segment['entity']:\n",
    "            if prev_segment['end'] == segment['start']:\n",
    "                entities[-1]['word'] += segment['word']\n",
    "                appended_to_prev_segment = True\n",
    "            elif _text[prev_segment['end']:segment['start']].isspace():\n",
    "                entities[-1]['word'] += _text[prev_segment['end']:segment['start']] + segment['word']\n",
    "                appended_to_prev_segment = True\n",
    "\n",
    "            if appended_to_prev_segment:\n",
    "                entities[-1]['end'] = segment['end']\n",
    "                entities[-1]['score'] = (entities[-1]['score'] + segment['score'])/2\n",
    "\n",
    "        if not appended_to_prev_segment:\n",
    "            original_entity_word = entity_word\n",
    "            entity_word = entity_word.lstrip()\n",
    "            segment['start'] = segment['start'] + (len(original_entity_word) - len(entity_word))\n",
    "            entities.append({\n",
    "                'entity': segment['entity'],\n",
    "                'word': entity_word,\n",
    "                'score': segment['score'],\n",
    "                'start': segment['start'],\n",
    "                'end': segment['end']\n",
    "            })\n",
    "        entities[-1]['word'] = re.sub(r' +', ' ', entities[-1]['word']).strip()\n",
    "        prev_segment = segment.copy()\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnEU",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_same_entities(text, raw_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ulZA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.trial_2.text_filteration import ContentFilterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_filter = ContentFilterer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZBYS",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_df = df.iloc[:200]\n",
    "# selected_df = selected_df._append(df.loc[df['event_id'] == 'case_1'])\n",
    "# selected_df = selected_df._append(df.loc[df['event_id'] == 'case_2'])\n",
    "# selected_df = selected_df._append(df.loc[df['event_id'] == 'case_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHfw",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering contents with CCF...: 100%|██████████| 354/354 [01:42<00:00,  3.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# cleaned_contents = []\n",
    "\n",
    "# # For cleaning all the contents\n",
    "# # for idx, row in tqdm(df.iterrows(), desc=\"Filtering contents with CCF...\", total=len(df)):\n",
    "# #     cleaned_content = content_filter.filter_text(row['title'], row['content'])\n",
    "# #     cleaned_contents.append(cleaned_content)\n",
    "\n",
    "# # For cleaning of selected sample set\n",
    "# for idx, row in tqdm(df.iterrows(), desc=\"Filtering contents with CCF...\", total=len(df)):\n",
    "#     cleaned_content = content_filter.filter_text(row['title'], row['content'])\n",
    "#     cleaned_contents.append(cleaned_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc8b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(\"cleaned_contents.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(cleaned_contents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba6f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"cleaned_contents.pkl\", \"rb\") as f:\n",
    "    cleaned_contents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AjVT",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample of keyword extraction & NER\n",
    "# _sample_text = \" \".join(cleaned_contents[19])\n",
    "# print(_sample_text)\n",
    "# sample_keywords = kw_extractor.extract_keywords(_sample_text)\n",
    "# print(sample_keywords)\n",
    "# sample_entities = combine_same_entities(_sample_text, fair_ner_pipe(_sample_text))\n",
    "# sample_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pHFh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_non_entity_keywords(_keywords, _entities):\n",
    "    entity_list = set([entity['word'] for entity in _entities])\n",
    "    _keywords = [keyword for keyword in _keywords if keyword[0] not in entity_list]\n",
    "    return _keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NCOB",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to calculate similarity between two strings\n",
    "def calculate_similarity(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "# Function to create a similarity matrix for all words\n",
    "def create_similarity_matrix(primary_word_list, secondary_word_list):\n",
    "    height = len(primary_word_list)\n",
    "    width = len(secondary_word_list)\n",
    "    matrix = [[0] * width for _ in range(height)]\n",
    "\n",
    "    for i, word1 in enumerate(primary_word_list):\n",
    "        for j, word2 in enumerate(secondary_word_list):\n",
    "            if i != j:  # Avoid self-comparison\n",
    "                matrix[i][j] = calculate_similarity(word1, word2)\n",
    "    return matrix\n",
    "\n",
    "# Function to group words based on similarity matrix\n",
    "def group_similar_words(word_list, top_values_threshold=None):\n",
    "    similarity_matrix = np.array(create_similarity_matrix(word_list, word_list))\n",
    "    # Dynamic thresholding \n",
    "    top_n_values = len(similarity_matrix[similarity_matrix>top_values_threshold]) if top_values_threshold is not None else similarity_matrix.shape[0]//1\n",
    "    top_n_values = 1 if top_n_values==0 else top_n_values\n",
    "    top_n_values *= -1\n",
    "    top_n_similarities = np.partition(similarity_matrix.flatten(), top_n_values)[top_n_values:]\n",
    "    threshold = np.mean(top_n_similarities)\n",
    "    print(f\"Threshold set to {threshold}\")\n",
    "    groups = []\n",
    "    visited = set()\n",
    "\n",
    "    for i, word in enumerate(word_list):\n",
    "        if word not in visited:\n",
    "            group = set()\n",
    "            for j, similarity in enumerate(similarity_matrix[i]):\n",
    "                if similarity >= threshold:\n",
    "                    group.add(word_list[j])\n",
    "            group.add(word)  # Include the current word\n",
    "            groups.append(list(group))\n",
    "            visited.update(group)  # Mark all words in this group as visited\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aqbW",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_kw_groups = group_similar_words([keyword[0] for keyword in sample_keywords], top_values_threshold=0.3)\n",
    "# sample_kw_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b5603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SequenceMatcher(None, sample_kw_groups[0][1], sample_entities[2]['word']).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcff1256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sample_kw_groups[0][1])\n",
    "# print(sample_entities[2]['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42690fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_same_entities(entities, threshold=0.55):\n",
    "    entity_word_list = [entity['word'] for entity in entities]\n",
    "    entity_similarities = np.array(create_similarity_matrix(entity_word_list, entity_word_list))\n",
    "    entity_groups = []\n",
    "    group_id = 0\n",
    "    entity_group_ids = [None] * len(entities)\n",
    "    is_grouped = [False] * len(entities)\n",
    "    for i, entity in enumerate(entities):\n",
    "        if not is_grouped[i]:\n",
    "            group = [entity]\n",
    "            is_grouped[i] = True\n",
    "            entity_group_ids[i] = group_id\n",
    "            shorlisted_similarities = (entity_similarities[i, :] > threshold)\n",
    "            for j, is_shorlisted in enumerate(shorlisted_similarities):\n",
    "                if not is_shorlisted or is_grouped[j]:\n",
    "                    continue\n",
    "                if entity['entity'] == entities[j]['entity']:\n",
    "                    group.append(entities[j])\n",
    "                    is_grouped[j] = True\n",
    "                    entity_group_ids[j] = group_id\n",
    "            entity_groups.append(group)\n",
    "            group_id += 1\n",
    "    return entity_groups, entity_group_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fda8c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_entity_groups, sample_entity_group_ids = identify_same_entities(entities=sample_entities, threshold=0.55)\n",
    "# sample_entity_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9703e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Entity:\n",
    "    name: str\n",
    "    entity_type: str\n",
    "    score: np.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6388102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_entity_relevant_keyword_groups(keyword_groups, entity_groups, entity_group_ids):\n",
    "  entity_keyword_group_ids = dict() # Initialize dictionary to store keyword group ids for each entity group\n",
    "  max_keyword_group_length = max([len(keyword_group) for keyword_group in keyword_groups]) # Get maximum number of keywords in a group\n",
    "  keyword_lengths = np.array([[len(keyword) for keyword in keyword_group] + ([-1]*(max_keyword_group_length-len(keyword_group))) for keyword_group in keyword_groups]) # Get lengths of all keywords in each group\n",
    "  for group_id, entity_group in enumerate(entity_groups): # Iterate over each entity group\n",
    "    entity_keyword_group_ids[group_id] = [] # Initialize list to store keyword group ids for the entity group\n",
    "    entity_words = [entity['word'] for entity in entity_group] # Get all words in the entity group\n",
    "    entity_word_lengths = [len(entity['word']) for entity in entity_group] # Get lengths of all words in the entity group\n",
    "    compared_entity_word = entity_words[np.argmax(entity_word_lengths)] # Get the longest word in the entity group\n",
    "    compared_entity_word_length = len(compared_entity_word) # Get the length of the longest word in the entity group\n",
    "    # Threshold need to be non-linearly inversed propotional (the more length of the word, the less propotion of the length of the word as threshold, the threshold = length x propotion, the propotion should be between 0 and 1)\n",
    "    default_proportion = 0.7 # For 3 words\n",
    "    acceptable_keyword_length_threshold = compared_entity_word_length * (default_proportion / math.log(compared_entity_word_length + 1, 10)) # Non-linearly inversed propotional threshold\n",
    "    acceptable_keyword_length_threshold = 1 if acceptable_keyword_length_threshold < 1 else acceptable_keyword_length_threshold # Minimum threshold to 1 (in case of very short words)\n",
    "    acceptable_keyword_length = [compared_entity_word_length - acceptable_keyword_length_threshold, compared_entity_word_length + acceptable_keyword_length_threshold] # Acceptable length range for keywords with the threshold\n",
    "    shortlisted_keyword_groups = [any(group_checks) for group_checks in np.bitwise_and(keyword_lengths >= acceptable_keyword_length[0], keyword_lengths <= acceptable_keyword_length[1])] # Shortlist groups with atleast one keyword in the acceptable length range\n",
    "    for i, is_shortlisted in enumerate(shortlisted_keyword_groups): # Iterate over shortlisted keyword groups\n",
    "        # Exit if the keyword group is not shortlisted\n",
    "        if not is_shortlisted:\n",
    "            continue\n",
    "        # Check if the entity group is relevant to the keyword group\n",
    "        similarity_scores = [SequenceMatcher(None, entity_word, keyword).ratio() for entity_word in entity_words for keyword in keyword_groups[i]]\n",
    "        if max(similarity_scores) == 1 or np.mean(similarity_scores) > 0.8:\n",
    "            entity_keyword_group_ids[group_id].append(i)\n",
    "    entity_keyword_group_ids[group_id] = None if len(entity_keyword_group_ids[group_id]) == 0 else entity_keyword_group_ids[group_id] # Set to None if no keyword group is relevant to the entity group\n",
    "  return entity_keyword_group_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc00bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_keyword_group_links = link_entity_relevant_keyword_groups(sample_kw_groups, sample_entity_groups, sample_entity_group_ids)\n",
    "# sample_entity_group_word_list = [[entity['word'] for entity in entity_group] for entity_group in sample_entity_groups]\n",
    "# { max(sample_entity_group_word_list[entity_group_id], key=len): None if relevent_kw_group_ids is None else [sample_kw_groups[kw_group_id] for kw_group_id in relevent_kw_group_ids] for entity_group_id, relevent_kw_group_ids in entity_keyword_group_links.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c3d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_entity_group_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b525cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max(sample_entity_group_word_list[0], key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08901048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_kw_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfdae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For the moment flatten the ner and keyword groups by length\n",
    "# proceeding_entity_list = [max(entity_group, key=len) for entity_group in sample_entity_group_word_list]\n",
    "# proceeding_keyword_list = [max(keyword_group, key=len) for keyword_group in sample_kw_groups]\n",
    "# proceeding_keyword_list = list(set(proceeding_keyword_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e4769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proceeding_keyword_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f09f46",
   "metadata": {},
   "source": [
    "### Article Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2303906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity:\n",
    "    entity: str\n",
    "    word: str\n",
    "    score: float\n",
    "    start: int\n",
    "    end: int\n",
    "\n",
    "    def __init__(self, _mapping: dict):\n",
    "        self.entity = _mapping['entity']\n",
    "        self.word = _mapping['word']\n",
    "        self.score = _mapping['score']\n",
    "        self.start = _mapping['start']\n",
    "        self.end = _mapping['end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8cae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class GroupKeyword:\n",
    "#     name: str\n",
    "#     threshold: float\n",
    "#     existence_rate: float\n",
    "\n",
    "# @dataclass\n",
    "# class GroupEntity(GroupKeyword):\n",
    "#     entity_type: str\n",
    "\n",
    "@dataclass\n",
    "class WordItem:\n",
    "  name: str\n",
    "  threshold: float\n",
    "  importance_score: float  # Track importance\n",
    "  frequency: int = 1  # Track occurrences\n",
    "\n",
    "@dataclass\n",
    "class GroupKeyword:\n",
    "  existence_rate: float\n",
    "  items: list[WordItem]\n",
    "\n",
    "@dataclass\n",
    "class GroupEntity(GroupKeyword):\n",
    "  entity_type: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de93ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article group utility functions\n",
    "def get_entity_similarity_matrix(primary_entity_list: list[GroupEntity], secondary_entity_list: list[Entity], top_n: int = None) -> np.ndarray:\n",
    "  \"\"\"Calculates the similarity between the group entity list and article entity list.\n",
    "\n",
    "  Args:\n",
    "      primary_entity_list (list[GroupEntity]): The list of group entities.\n",
    "      secondary_entity_list (list[Entity]): The list of article entities.\n",
    "      top_n (int, optional): The number of top entities(per entity group) to consider. Defaults to None (the square root of the no. of entities per group).\n",
    "\n",
    "  Returns:\n",
    "      np.ndarray: A matrix representing entity similarity scores.\n",
    "  \"\"\"\n",
    "  # Get the top N count list to consider\n",
    "  if top_n is not None:\n",
    "    assert top_n > 0, \"Top N must be greater than 0\"\n",
    "    assert all([top_n > len(entity_list.items) for entity_list in primary_entity_list]), \"Top N must be greater than the number of entities in every group\"\n",
    "    top_n_list = [top_n] * len(primary_entity_list)\n",
    "  else:\n",
    "    top_n_list = [int(np.sqrt(len(entity_list.items))) for entity_list in primary_entity_list]\n",
    "  \n",
    "  # Create the similarity matrix\n",
    "  group_similarity_matrices = []\n",
    "  for i, entity_list in enumerate(primary_entity_list):\n",
    "    top_n_entities = sorted(entity_list.items, key=lambda x: x.importance_score, reverse=True)[:top_n_list[i]]\n",
    "    group_similarity_matrix = np.array(create_similarity_matrix(\n",
    "      [entity.name for entity in top_n_entities],\n",
    "      [entity.word for entity in secondary_entity_list]\n",
    "    ))\n",
    "    group_similarity_matrix = np.mean(group_similarity_matrix, axis=0)\n",
    "    group_similarity_matrices.append(group_similarity_matrix)\n",
    "  group_similarity_matrices = np.array(group_similarity_matrices)\n",
    "  # Create the entity type match matrix\n",
    "  type_match_matrix = np.array([[1 if entity.entity_type == entity2.entity else 0 for entity2 in secondary_entity_list] for entity in primary_entity_list])\n",
    "  # Match the entity type matrix with the similarity matrix\n",
    "  assert group_similarity_matrices.shape == type_match_matrix.shape # Ensure the shape of both matrices are the same\n",
    "  final_score_matrix = group_similarity_matrices * type_match_matrix\n",
    "  return final_score_matrix\n",
    "\n",
    "def get_entity_type_matrix(primary_entity_list: list[GroupEntity], secondary_entity_list: list[Entity], entity_similarity_matrix: np.ndarray) -> np.ndarray:\n",
    "  no_of_article_entities = len(secondary_entity_list)\n",
    "  entity_type_matrix = []\n",
    "  for i, entity_group in enumerate(primary_entity_list):\n",
    "    try:\n",
    "        max_index = np.argmax(entity_similarity_matrix[i])\n",
    "    except Exception:\n",
    "        print(entity_similarity_matrix)\n",
    "        raise\n",
    "    weighted_threshold = np.average([entity.threshold for entity in entity_group.items], weights=[entity.importance_score for entity in entity_group.items])\n",
    "    if entity_similarity_matrix[i, max_index] < weighted_threshold:\n",
    "      entity_type_matrix.append([None] * no_of_article_entities)\n",
    "    else:\n",
    "      entity_type_matrix.append([None] * max_index + [entity_group.entity_type] + [None] * (no_of_article_entities - max_index - 1))\n",
    "  entity_type_matrix = np.array(entity_type_matrix)\n",
    "  return entity_type_matrix\n",
    "\n",
    "def get_kw_group_match_matrix(primary_kw_group_list: list[GroupKeyword], secondary_kw_list: list[str], top_n: int = None, match_threshold: float = 0.5) -> np.ndarray:\n",
    "    \"\"\"Calculates the match matrix between group keywords and article keywords.\n",
    "\n",
    "    Args:\n",
    "        primary_kw_group_list (list[GroupKeyword]): The list of group keywords.\n",
    "        secondary_kw_list (list[str]): The list of article keywords.\n",
    "        top_n (int, optional): The number of top keywords(per keyword group) to consider. \n",
    "            Defaults to None (the square root of the no. of keywords per group).\n",
    "        match_threshold (float, optional): The minimum proportion of matches required \n",
    "            among top_n keywords. Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A matrix where each cell represents either 0 (no match) or \n",
    "                   a match rate (if matched) for a keyword group.\n",
    "    \"\"\"\n",
    "    # Determine top_n for each keyword group\n",
    "    if top_n is not None:\n",
    "        assert top_n > 0, \"Top N must be greater than 0\"\n",
    "        assert all([top_n <= len(keyword_group.items) for keyword_group in primary_kw_group_list\n",
    "        ]), \"Top N must be less than or equal to the number of keywords in every group\"\n",
    "        top_n_list = [top_n] * len(primary_kw_group_list)\n",
    "    else:\n",
    "        top_n_list = [max(2, int(np.sqrt(len(keyword_group.items)))) for keyword_group in primary_kw_group_list]\n",
    "\n",
    "    # Create the match matrix\n",
    "    group_match_matrices = []\n",
    "    \n",
    "    for i, keyword_group in enumerate(primary_kw_group_list):\n",
    "        # Sort by importance_score instead of context_score\n",
    "        top_n_keywords = sorted(keyword_group.items, key=lambda x: x.importance_score, reverse=True)[:top_n_list[i]]\n",
    "        \n",
    "        # Create a matrix of binary matches (0 or 1) based on thresholds\n",
    "        keyword_matches = []\n",
    "        for keyword in top_n_keywords:\n",
    "            # Check if any secondary keyword matches this word item based on its threshold\n",
    "            match_vector = np.zeros(len(secondary_kw_list))\n",
    "            \n",
    "            for j, sec_keyword in enumerate(secondary_kw_list):\n",
    "                # Use exact match or implement a similarity function here\n",
    "                similarity = calculate_similarity(keyword.name, sec_keyword)\n",
    "                if similarity >= keyword.threshold:\n",
    "                    match_vector[j] = 1\n",
    "                    \n",
    "            keyword_matches.append(match_vector)\n",
    "        \n",
    "        keyword_matches = np.array(keyword_matches)\n",
    "        \n",
    "        # Calculate match rate for this group across all secondary keywords\n",
    "        if len(keyword_matches) > 0:\n",
    "            # Method 1: For each secondary keyword, check if enough top keywords are matched\n",
    "            match_counts = np.sum(keyword_matches, axis=0)\n",
    "            min_matches_needed = max(1, int(len(top_n_keywords) * match_threshold))\n",
    "            \n",
    "            # Binary decision for each secondary keyword\n",
    "            group_match_vector = np.zeros(len(secondary_kw_list))\n",
    "            for j in range(len(secondary_kw_list)):\n",
    "                if match_counts[j] >= min_matches_needed:\n",
    "                    # Calculate match rate: weighted by importance and existence rate\n",
    "                    matched_keywords = [top_n_keywords[k] for k in range(len(top_n_keywords)) if keyword_matches[k, j] == 1]\n",
    "                    \n",
    "                    # Method for match rate: weighted average of matched keywords' importance\n",
    "                    total_importance = sum(kw.importance_score for kw in matched_keywords)\n",
    "                    max_possible = sum(kw.importance_score for kw in top_n_keywords[:min_matches_needed])\n",
    "                    \n",
    "                    # Scale by the group's existence rate\n",
    "                    match_rate = (total_importance / max_possible) * keyword_group.existence_rate\n",
    "                    \n",
    "                    group_match_vector[j] = match_rate\n",
    "        else:\n",
    "            group_match_vector = np.zeros(len(secondary_kw_list))\n",
    "            \n",
    "        group_match_matrices.append(group_match_vector)\n",
    "    \n",
    "    return np.array(group_match_matrices)\n",
    "\n",
    "def calculate_threshold(phrase, base=0.7, len_penalty=0.01, token_penalty=0.05):\n",
    "  \"\"\"Calculates the threshold for a given phrase.\n",
    "\n",
    "  Args:\n",
    "      phrase (str): The phrase to calculate the threshold for.\n",
    "      base (float, optional): The base threshold. Defaults to 0.7.\n",
    "      len_penalty (float, optional): The length penalty. Defaults to 0.01.\n",
    "      token_penalty (float, optional): The token penalty. Defaults to 0.05.\n",
    "\n",
    "  Returns:\n",
    "      float: The calculated threshold\n",
    "  \"\"\"\n",
    "  tokens = phrase.split()\n",
    "  length = len(phrase)\n",
    "  num_tokens = len(tokens)\n",
    "  threshold = base - (len_penalty * length) - (token_penalty * (num_tokens - 1))\n",
    "  return max(0.2, min(0.9, threshold))  # Clamp between 0.2 and 0.9\n",
    "\n",
    "def calculate_importance_score(*, initial: bool = False, total_articles: int | None = None, similarity_score: float | None = None, importance_score: float | None = None, frequency: int = 1) -> float:\n",
    "  \"\"\"Calculates the importance score for a given article.\n",
    "\n",
    "  Args:\n",
    "      initial (bool, optional): Indicates if this is the initial calculation. Defaults to False.\n",
    "      total_articles (int | None, optional): The total number of articles. Defaults to None.\n",
    "      similarity_score (float | None, optional): The similarity score related to the article. Defaults to None.\n",
    "      importance_score (float | None, optional): The current importance score. Defaults to None.\n",
    "      frequency (int, optional): The frequency of the article's appearance. Defaults to 1.\n",
    "\n",
    "  Raises:\n",
    "      ValueError: If total_articles is not provided.\n",
    "      ValueError: If similarity_score is not provided for initial importance score calculation.\n",
    "      ValueError: If importance_score is not provided for importance score calculation.\n",
    "      ValueError: If frequency is not provided for initial importance score calculation.\n",
    "      ValueError: If frequency is greater than total articles.\n",
    "\n",
    "  Returns:\n",
    "      float: The calculated importance score\n",
    "  \"\"\"\n",
    "  if total_articles is None:\n",
    "      raise ValueError(\"Total articles must be provided\")\n",
    "  \n",
    "  if initial:\n",
    "    if similarity_score is None:\n",
    "        raise ValueError(\"Similarity score must be provided for initial importance score calculation\")\n",
    "  else:\n",
    "    if importance_score is None:\n",
    "        raise ValueError(\"Importance score must be provided for importance score calculation\")\n",
    "  \n",
    "  if frequency is None:\n",
    "      raise ValueError(\"Frequency must be provided for initial importance score calculation\")\n",
    "  elif frequency > total_articles:\n",
    "      raise ValueError(\"Frequency must be less than total articles\")\n",
    "  \n",
    "  if initial:\n",
    "      return similarity_score * (frequency / total_articles)\n",
    "  else:\n",
    "      # Update the importance with the new article addition (use importance_score, frequency and total_articles only)\n",
    "      return importance_score + (frequency / total_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d962a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_weights = {\n",
    "  \"I-PER\": 0.4, # Person\n",
    "  \"I-LOC\": 0.5, # Location\n",
    "  \"I-ORG\": 0.3, # Organization\n",
    "  \"I-MISC\": 0.2 # Miscellaneous\n",
    "}\n",
    "\n",
    "class ArticleGroup:\n",
    "  identifier: int\n",
    "  __internal_entities: list[GroupEntity]\n",
    "  __internal_keywords: list[GroupKeyword]\n",
    "  __group_article_ids: list[str]\n",
    "  __total_entity_scorable: float\n",
    "\n",
    "  def __init__(self, identifier: int = -1):\n",
    "    self.identifier = identifier\n",
    "    self.__group_article_ids = []\n",
    "    self.__internal_entities = []\n",
    "    self.__total_entity_scorable = 0\n",
    "    self.__internal_keywords = []\n",
    "\n",
    "  def get_group_article_ids(self):\n",
    "    return self.__group_article_ids\n",
    "  \n",
    "  def get_group_keywords(self):\n",
    "    return self.__internal_keywords\n",
    "  \n",
    "  def get_group_entities(self):\n",
    "    return self.__internal_entities\n",
    "  \n",
    "  def __add_article_id(self, article_id: str):\n",
    "    self.__group_article_ids.append(article_id)\n",
    "  \n",
    "  def add_initial_article(self, article_id: str, entities: list[dict], keywords: list[str]):\n",
    "    \"\"\"\n",
    "    Add the first article to an empty article group, initializing its entities and keywords.\n",
    "\n",
    "    Args:\n",
    "        article_id (str): Unique identifier for the article\n",
    "        entities (list[dict]): List of entities with 'word' and 'entity' keys\n",
    "        keywords (list[str]): List of keywords from the article\n",
    "    \"\"\"\n",
    "    global entity_weights\n",
    "\n",
    "    # Initialize entities\n",
    "    for entity in entities:\n",
    "        # Create a new entity group for each unique entity type\n",
    "        entity_group = GroupEntity(\n",
    "            items=[],\n",
    "            entity_type=entity.entity,\n",
    "            existence_rate=1.0  # First article, so full existence rate\n",
    "        )\n",
    "        \n",
    "        # Create a word item for the entity\n",
    "        word_item = WordItem(\n",
    "            name=entity.word.lower(),\n",
    "            frequency=1,\n",
    "            threshold=calculate_threshold(entity.word),\n",
    "            importance_score=1.0  # Maximum initial importance\n",
    "        )\n",
    "        \n",
    "        # Add word item to the entity group\n",
    "        entity_group.items.append(word_item)\n",
    "        \n",
    "        # Add entity group to internal entities\n",
    "        self.__internal_entities.append(entity_group)\n",
    "\n",
    "    # Calculate total entity scorable\n",
    "    self.__total_entity_scorable = sum([entity_weights[entity.entity] for entity in entities])\n",
    "\n",
    "    # Initialize keywords\n",
    "    for keyword in keywords:\n",
    "        # Create a new keyword group\n",
    "        keyword_group = GroupKeyword(\n",
    "            items=[],\n",
    "            existence_rate=1.0  # First article, so full existence rate\n",
    "        )\n",
    "        \n",
    "        # Create a word item for the keyword\n",
    "        word_item = WordItem(\n",
    "            name=keyword.lower(),\n",
    "            frequency=1,\n",
    "            threshold=calculate_threshold(keyword),\n",
    "            importance_score=1.0  # Maximum initial importance\n",
    "        )\n",
    "        \n",
    "        # Add word item to the keyword group\n",
    "        keyword_group.items.append(word_item)\n",
    "        \n",
    "        # Add keyword group to internal keywords\n",
    "        self.__internal_keywords.append(keyword_group)\n",
    "\n",
    "    # Set total keyword scorable\n",
    "    self.__total_keyword_scorable = len(keywords)\n",
    "\n",
    "    # Add the article ID to the group\n",
    "    self.__add_article_id(article_id)\n",
    "\n",
    "#   def add_article(self, article_id: str, entities: list[Entity], keywords: list[str]):\n",
    "#     # If this is the first article in the group, use add_initial_article\n",
    "#     if len(self.__group_article_ids) == 0:\n",
    "#         self.add_initial_article(article_id, entities, keywords)\n",
    "#         return\n",
    "\n",
    "#     global entity_weights\n",
    "#     no_of_articles = len(self.__group_article_ids)\n",
    "#     # Mutate the list of entities with the new entities\n",
    "#     ## Get entity similarity score matrix\n",
    "#     entity_similarity_matrix = get_entity_similarity_matrix(self.__internal_entities, entities)\n",
    "#     ## Create entity type matrix\n",
    "#     entity_type_matrix = get_entity_type_matrix(self.__internal_entities, entities, entity_similarity_matrix)\n",
    "#     ## Get Matched/Non-existent status of entities\n",
    "#     is_matched = np.any(entity_type_matrix, axis=1)\n",
    "#     for i, entity_type_matrix_col in enumerate(entity_type_matrix.T):\n",
    "#       ## Update matched entity groups\n",
    "#       if is_matched[i]:\n",
    "#         matched_entity_group_index = np.argmax(entity_type_matrix[:, i] is not None)\n",
    "#         ### Check if the entity word already exists\n",
    "#         found_match = False\n",
    "#         for existing_entity in self.__internal_entities[matched_entity_group_index].items:\n",
    "#           if existing_entity.name == entities[i].word.lower():\n",
    "#             existing_entity.frequency += 1\n",
    "#             existing_entity.importance_score=calculate_importance_score(\n",
    "#                 initial=False,\n",
    "#                 importance_score=existing_entity.importance_score,\n",
    "#                 frequency=existing_entity.frequency,\n",
    "#                 total_articles=no_of_articles\n",
    "#             )\n",
    "#             found_match = True\n",
    "#             break\n",
    "#         if not found_match:\n",
    "#           entity = WordItem(\n",
    "#             name=entities[i].word,\n",
    "#             threshold=calculate_threshold(entities[i].word),\n",
    "#             importance_score=calculate_importance_score(\n",
    "#                 initial=True,\n",
    "#                 similarity_score=entity_similarity_matrix[matched_entity_group_index, i], \n",
    "#                 frequency=1,\n",
    "#                 total_articles=no_of_articles\n",
    "#             )\n",
    "#           )\n",
    "#           self.__internal_entities[matched_entity_group_index].items.append(entity)\n",
    "#     ## Add non-existent entities\n",
    "#     ## Pick non-existent entities (to a new entity group) with the help of the entity type matrix\n",
    "#     for i, entity_type_matrix_col in enumerate(entity_type_matrix.T):\n",
    "#       if not any(entity_type_matrix_col):\n",
    "#         entity_group = GroupEntity(\n",
    "#           entity_type=entities[i].entity,\n",
    "#           existence_rate=0,  # Calculated later\n",
    "#           items=[]\n",
    "#         )\n",
    "#         entity = WordItem(\n",
    "#           name=entities[i].word,\n",
    "#           frequency=1,\n",
    "#           threshold=calculate_threshold(entities[i].word),\n",
    "#           importance_score=1\n",
    "#         )\n",
    "#         entity_group.items.append(entity)\n",
    "#         self.__internal_entities.append(entity_group)\n",
    "#     ## Re-calculate existence rate of the entities\n",
    "#     new_no_of_articles = no_of_articles + 1\n",
    "#     is_updated = np.any(entity_type_matrix is not None).tolist()\n",
    "#     is_updated = [is_updated] if isinstance(is_updated, bool) else is_updated\n",
    "#     no_of_added_groups = len(self.__internal_entities) - len(is_updated)\n",
    "#     is_updated.extend([True]*no_of_added_groups)\n",
    "#     for entity_group, is_modded in zip(self.__internal_entities, is_updated):\n",
    "#         eq_constant = 1 if is_modded else 0\n",
    "#         entity_group.existence_rate = ((entity_group.existence_rate * no_of_articles) + eq_constant) / new_no_of_articles\n",
    "      \n",
    "#     ## Total entity scorable calculation\n",
    "#     self.__total_entity_scorable = sum([entity_weights[entity.entity] for entity in entities])\n",
    "    \n",
    "#     # Mutate the list of keywords with the new keywords\n",
    "#     ## Get keyword match matrix\n",
    "#     kw_match_matrix = get_kw_group_match_matrix(self.__internal_keywords, keywords)\n",
    "    \n",
    "#     ## Update matched keyword groups\n",
    "#     for i in range(len(keywords)):\n",
    "#         # For each keyword, find all groups that matched with it\n",
    "#         matched_groups = []\n",
    "#         for j in range(len(self.__internal_keywords)):\n",
    "#             if kw_match_matrix[j, i] > 0:\n",
    "#                 matched_groups.append((j, kw_match_matrix[j, i]))  # Store (group_index, match_rate)\n",
    "        \n",
    "#         if matched_groups:\n",
    "#             # Sort by match rate to find the best matching group\n",
    "#             matched_groups.sort(key=lambda x: x[1], reverse=True)\n",
    "#             best_match_group_index = matched_groups[0][0]\n",
    "            \n",
    "#             ### Check if the keyword already exists in the best matching group\n",
    "#             found_match = False\n",
    "#             for existing_keyword in self.__internal_keywords[best_match_group_index].items:\n",
    "#                 if existing_keyword.name.lower() == keywords[i].lower():\n",
    "#                     existing_keyword.frequency += 1\n",
    "#                     existing_keyword.importance_score = calculate_importance_score(\n",
    "#                         initial=False,\n",
    "#                         importance_score=existing_keyword.importance_score,\n",
    "#                         frequency=existing_keyword.frequency,\n",
    "#                         total_articles=no_of_articles\n",
    "#                     )\n",
    "#                     found_match = True\n",
    "#                     break\n",
    "            \n",
    "#             if not found_match:\n",
    "#                 # Add the keyword to the best matching group\n",
    "#                 keyword = WordItem(\n",
    "#                     name=keywords[i],\n",
    "#                     threshold=calculate_threshold(keywords[i]),\n",
    "#                     importance_score=calculate_importance_score(\n",
    "#                         initial=True,\n",
    "#                         similarity_score=matched_groups[0][1],  # Use match rate as similarity score\n",
    "#                         frequency=1,\n",
    "#                         total_articles=no_of_articles\n",
    "#                     )\n",
    "#               )\n",
    "#                 self.__internal_keywords[best_match_group_index].items.append(keyword)\n",
    "    \n",
    "#     # Get indices of keywords that didn't match any group\n",
    "#     non_matched_keywords = [i for i in range(len(keywords)) if not any(kw_match_matrix[:, i] > 0)]\n",
    "    \n",
    "#     # Create new keyword groups for non-matched keywords\n",
    "#     for i in non_matched_keywords:\n",
    "#         keyword_group = GroupKeyword(\n",
    "#             items=[],\n",
    "#             existence_rate=0  # Will be calculated later\n",
    "#         )\n",
    "#         keyword = WordItem(\n",
    "#             name=keywords[i],\n",
    "#             frequency=1,\n",
    "#             threshold=calculate_threshold(keywords[i]),\n",
    "#             importance_score=1  # Initial importance score\n",
    "#         )\n",
    "#         keyword_group.items.append(keyword)\n",
    "#         self.__internal_keywords.append(keyword_group)\n",
    "  \n",
    "#     ## Re-calculate existence rate of the keywords\n",
    "#     new_no_of_articles = no_of_articles + 1\n",
    "#     is_kw_updated = np.any(kw_match_matrix > 0, axis=1).tolist()\n",
    "#     is_kw_updated = [is_kw_updated] if isinstance(is_kw_updated, bool) else is_kw_updated\n",
    "#     no_of_added_kw_groups = len(self.__internal_keywords) - len(is_kw_updated)\n",
    "#     is_kw_updated.extend([True] * no_of_added_kw_groups)\n",
    "    \n",
    "#     for keyword_group, is_modded in zip(self.__internal_keywords, is_kw_updated):\n",
    "#         eq_constant = 1 if is_modded else 0\n",
    "#         keyword_group.existence_rate = ((keyword_group.existence_rate * no_of_articles) + eq_constant) / new_no_of_articles\n",
    "\n",
    "#     ## Total keyword scorable calculation - similar concept to entity scorable\n",
    "#     self.__total_keyword_scorable = len(keywords)  # Simple count, could be weighted if needed\n",
    "    \n",
    "#     # Add the article ID to the group\n",
    "#     self.__add_article_id(article_id)\n",
    "\n",
    "  def add_article(self, article_id: str, entities: list[Entity], keywords: list[str]):\n",
    "    # Check if duplicate article\n",
    "    if article_id in self.__group_article_ids:\n",
    "        print(f\"Article {article_id} already exists in the group.\")\n",
    "        return\n",
    "    \n",
    "    # If this is the first article in the group, use add_initial_article\n",
    "    if len(self.__group_article_ids) == 0:\n",
    "        self.add_initial_article(article_id, entities, keywords)\n",
    "        return\n",
    "\n",
    "    global entity_weights\n",
    "    self.__add_article_id(article_id)\n",
    "    no_of_articles = len(self.__group_article_ids)\n",
    "    # Mutate the list of entities with the new entities\n",
    "    ## Get entity similarity score matrix\n",
    "    entity_similarity_matrix = get_entity_similarity_matrix(self.__internal_entities, entities)\n",
    "    ## Create entity type matrix\n",
    "    entity_type_matrix = get_entity_type_matrix(self.__internal_entities, entities, entity_similarity_matrix)\n",
    "    ## Get Matched/Non-existent status of entities\n",
    "    is_matched = np.any(entity_type_matrix, axis=0)\n",
    "    \n",
    "    # Track processed entities to avoid duplicate frequency increments\n",
    "    processed_entities = set()\n",
    "    \n",
    "    for i, entity_type_matrix_col in enumerate(entity_type_matrix.T):\n",
    "      ## Update matched entity groups\n",
    "      if is_matched[i]:\n",
    "        matched_entity_group_index = np.argmax(entity_type_matrix[:, i] is not None)\n",
    "        entity_word = entities[i].word.lower()\n",
    "        \n",
    "        # Create a unique key for this entity in this group\n",
    "        entity_key = f\"{matched_entity_group_index}:{entity_word}\"\n",
    "        \n",
    "        ### Check if the entity word already exists\n",
    "        found_match = False\n",
    "        \n",
    "        for existing_entity in self.__internal_entities[matched_entity_group_index].items:\n",
    "          if existing_entity.name == entity_word:\n",
    "            # Only increment frequency if this entity hasn't been processed yet for this article\n",
    "            if entity_key not in processed_entities:\n",
    "              existing_entity.frequency += 1\n",
    "              existing_entity.importance_score=calculate_importance_score(\n",
    "                  initial=False,\n",
    "                  importance_score=existing_entity.importance_score,\n",
    "                  frequency=existing_entity.frequency,\n",
    "                  total_articles=no_of_articles\n",
    "              )\n",
    "              # Mark this entity as processed for this article\n",
    "              processed_entities.add(entity_key)\n",
    "            found_match = True\n",
    "            break\n",
    "            \n",
    "        if not found_match:\n",
    "          entity = WordItem(\n",
    "            name=entities[i].word,\n",
    "            threshold=calculate_threshold(entities[i].word),\n",
    "            importance_score=calculate_importance_score(\n",
    "                initial=True,\n",
    "                similarity_score=entity_similarity_matrix[matched_entity_group_index, i], \n",
    "                frequency=1,\n",
    "                total_articles=no_of_articles\n",
    "            )\n",
    "          )\n",
    "          self.__internal_entities[matched_entity_group_index].items.append(entity)\n",
    "          # No need to add to processed_entities as we're creating a new entity with frequency 1\n",
    "          \n",
    "    ## Add non-existent entities\n",
    "    ## Pick non-existent entities (to a new entity group) with the help of the entity type matrix\n",
    "    for i, entity_type_matrix_col in enumerate(entity_type_matrix.T):\n",
    "      if not any(entity_type_matrix_col):\n",
    "        entity_group = GroupEntity(\n",
    "          entity_type=entities[i].entity,\n",
    "          existence_rate=0,  # Calculated later\n",
    "          items=[]\n",
    "        )\n",
    "        entity = WordItem(\n",
    "          name=entities[i].word,\n",
    "          frequency=1,\n",
    "          threshold=calculate_threshold(entities[i].word),\n",
    "          importance_score=1\n",
    "        )\n",
    "        entity_group.items.append(entity)\n",
    "        self.__internal_entities.append(entity_group)\n",
    "        # No need to add to processed_entities as we're creating a new entity group with frequency 1\n",
    "        \n",
    "    ## Re-calculate existence rate of the entities\n",
    "    new_no_of_articles = no_of_articles + 1\n",
    "    is_updated = np.any(entity_type_matrix is not None).tolist()\n",
    "    is_updated = [is_updated] if isinstance(is_updated, bool) else is_updated\n",
    "    no_of_added_groups = len(self.__internal_entities) - len(is_updated)\n",
    "    is_updated.extend([True]*no_of_added_groups)\n",
    "    for entity_group, is_modded in zip(self.__internal_entities, is_updated):\n",
    "        eq_constant = 1 if is_modded else 0\n",
    "        entity_group.existence_rate = ((entity_group.existence_rate * no_of_articles) + eq_constant) / new_no_of_articles\n",
    "      \n",
    "    ## Total entity scorable calculation\n",
    "    self.__total_entity_scorable = sum([entity_weights[entity.entity] for entity in entities])\n",
    "    \n",
    "    # Mutate the list of keywords with the new keywords\n",
    "    ## Get keyword match matrix\n",
    "    kw_match_matrix = get_kw_group_match_matrix(self.__internal_keywords, keywords)\n",
    "    \n",
    "    # Track processed keywords to avoid duplicate frequency increments\n",
    "    processed_keywords = set()\n",
    "    \n",
    "    ## Update matched keyword groups\n",
    "    for i in range(len(keywords)):\n",
    "        keyword_lower = keywords[i].lower()\n",
    "        \n",
    "        # For each keyword, find all groups that matched with it\n",
    "        matched_groups = []\n",
    "        for j in range(len(self.__internal_keywords)):\n",
    "            if kw_match_matrix[j, i] > 0:\n",
    "                matched_groups.append((j, kw_match_matrix[j, i]))  # Store (group_index, match_rate)\n",
    "        \n",
    "        if matched_groups:\n",
    "            # Sort by match rate to find the best matching group\n",
    "            matched_groups.sort(key=lambda x: x[1], reverse=True)\n",
    "            best_match_group_index = matched_groups[0][0]\n",
    "            \n",
    "            # Create a unique key for this keyword in this group\n",
    "            keyword_key = f\"{best_match_group_index}:{keyword_lower}\"\n",
    "            \n",
    "            ### Check if the keyword already exists in the best matching group\n",
    "            found_match = False\n",
    "            for existing_keyword in self.__internal_keywords[best_match_group_index].items:\n",
    "                if existing_keyword.name.lower() == keyword_lower:\n",
    "                    # Only increment frequency if this keyword hasn't been processed yet for this article\n",
    "                    if keyword_key not in processed_keywords:\n",
    "                        existing_keyword.frequency += 1\n",
    "                        try:\n",
    "                          existing_keyword.importance_score = calculate_importance_score(\n",
    "                              initial=False,\n",
    "                              importance_score=existing_keyword.importance_score,\n",
    "                              frequency=existing_keyword.frequency,\n",
    "                              total_articles=no_of_articles\n",
    "                          )\n",
    "                        except Exception:\n",
    "                          print(f\"Error calculating importance score for keyword: {existing_keyword.name}\")\n",
    "                          print(f\"Frequency: {existing_keyword.frequency}, Total Articles: {no_of_articles}\")\n",
    "                          raise\n",
    "                        # Mark this keyword as processed for this article\n",
    "                        processed_keywords.add(keyword_key)\n",
    "                    found_match = True\n",
    "                    break\n",
    "            \n",
    "            if not found_match:\n",
    "                # Add the keyword to the best matching group\n",
    "                keyword = WordItem(\n",
    "                    name=keywords[i],\n",
    "                    threshold=calculate_threshold(keywords[i]),\n",
    "                    importance_score=calculate_importance_score(\n",
    "                        initial=True,\n",
    "                        similarity_score=matched_groups[0][1],  # Use match rate as similarity score\n",
    "                        frequency=1,\n",
    "                        total_articles=no_of_articles\n",
    "                    )\n",
    "                )\n",
    "                self.__internal_keywords[best_match_group_index].items.append(keyword)\n",
    "                # No need to add to processed_keywords as we're creating a new keyword with frequency 1\n",
    "    \n",
    "    # Get indices of keywords that didn't match any group\n",
    "    non_matched_keywords = [i for i in range(len(keywords)) if not any(kw_match_matrix[:, i] > 0)]\n",
    "    \n",
    "    # Create new keyword groups for non-matched keywords\n",
    "    for i in non_matched_keywords:\n",
    "        keyword_group = GroupKeyword(\n",
    "            items=[],\n",
    "            existence_rate=0  # Will be calculated later\n",
    "        )\n",
    "        keyword = WordItem(\n",
    "            name=keywords[i],\n",
    "            frequency=1,\n",
    "            threshold=calculate_threshold(keywords[i]),\n",
    "            importance_score=1  # Initial importance score\n",
    "        )\n",
    "        keyword_group.items.append(keyword)\n",
    "        self.__internal_keywords.append(keyword_group)\n",
    "        # No need to add to processed_keywords as we're creating a new keyword group with frequency 1\n",
    "  \n",
    "    ## Re-calculate existence rate of the keywords\n",
    "    new_no_of_articles = no_of_articles + 1\n",
    "    is_kw_updated = np.any(kw_match_matrix > 0).tolist()\n",
    "    is_kw_updated = [is_kw_updated] if isinstance(is_kw_updated, bool) else is_kw_updated\n",
    "    no_of_added_kw_groups = len(self.__internal_keywords) - len(is_kw_updated)\n",
    "    is_kw_updated.extend([True] * no_of_added_kw_groups)\n",
    "    \n",
    "    for keyword_group, is_modded in zip(self.__internal_keywords, is_kw_updated):\n",
    "        eq_constant = 1 if is_modded else 0\n",
    "        keyword_group.existence_rate = ((keyword_group.existence_rate * no_of_articles) + eq_constant) / new_no_of_articles\n",
    "\n",
    "    ## Total keyword scorable calculation - similar concept to entity scorable\n",
    "    self.__total_keyword_scorable = len(keywords)  # Simple count, could be weighted if needed\n",
    "    \n",
    "    # # Add the article ID to the group\n",
    "    # self.__add_article_id(article_id)\n",
    "\n",
    "\n",
    "  def get_group_relevance_score(self, entities: list[dict], keywords: list[str], \n",
    "                            entity_match_threshold: float = 0.8, \n",
    "                            keyword_match_threshold: float = 0.8, \n",
    "                            match_threshold: float = 0.7) -> bool:\n",
    "    \"\"\"Calculates the relevance score of the article group based on the entities and keywords.\n",
    "\n",
    "    Args:\n",
    "        entities (list[dict]): List of entity dictionaries with 'word' and 'entity' keys\n",
    "        keywords (list[str]): List of keywords from the article\n",
    "        entity_match_threshold (float, optional): Threshold for entity matching. Defaults to 0.8.\n",
    "        keyword_match_threshold (float, optional): Threshold for keyword matching. Defaults to 0.8.\n",
    "        match_threshold (float, optional): Overall match threshold. Defaults to 0.7.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether the article is relevant to the group\n",
    "    \"\"\"\n",
    "    global entity_weights\n",
    "    # Check entity relevance (40% weight)\n",
    "    ## Get entity similarity score matrix\n",
    "    entity_similarity_matrix = get_entity_similarity_matrix(self.__internal_entities, entities)\n",
    "    ## Create entity type matrix\n",
    "    try:\n",
    "        entity_type_matrix = get_entity_type_matrix(\n",
    "            self.__internal_entities, \n",
    "            entities, \n",
    "            entity_similarity_matrix\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating entity type matrix: {e}\")\n",
    "        print(f\"Internal Entities: {self.__internal_entities}\")\n",
    "        print(f\"Entities: {entities}\")\n",
    "        print(f\"Entity Similarity Matrix: {entity_similarity_matrix}\")\n",
    "        raise\n",
    "    ## Calculate entity weight matrix\n",
    "    entity_weight_matrix = np.array([\n",
    "        [entity_weights.get(entity_type, 0) if entity_type is not None else 0 \n",
    "        for entity_type in entity_type_matrix_row] \n",
    "        for entity_type_matrix_row in entity_type_matrix\n",
    "    ])\n",
    "    ## Calculate entity score\n",
    "    # Sum of weights for matched entities, scaled by their similarity and type weights\n",
    "    entity_score = np.sum(entity_weight_matrix)\n",
    "    ## Normalize entity score (40% weight)\n",
    "    # Prevent division by zero\n",
    "    normalized_entity_score = (\n",
    "        (entity_score / max(self.__total_entity_scorable, 1)) * 0.4 \n",
    "        if self.__total_entity_scorable > 0 \n",
    "        else 0\n",
    "    )\n",
    "    # Early exit if no entity match\n",
    "    if normalized_entity_score == 0:\n",
    "        return False\n",
    "    # Check keyword relevance (60% weight)\n",
    "    ## Use the new group match matrix\n",
    "    kw_match_matrix = get_kw_group_match_matrix(self.__internal_keywords, keywords)\n",
    "    # Calculate keyword score\n",
    "    # Sum of match rates across all keyword groups\n",
    "    keyword_score = np.sum(kw_match_matrix)\n",
    "    # Normalize keyword score (60% weight)\n",
    "    # Prevent division by zero and handle empty keywords\n",
    "    normalized_keyword_score = (\n",
    "        (keyword_score / max(self.__total_keyword_scorable, 1)) * 0.6 \n",
    "        if self.__total_keyword_scorable > 0 and keywords \n",
    "        else 0\n",
    "    )\n",
    "    # Calculate total score\n",
    "    total_score = normalized_entity_score + normalized_keyword_score\n",
    "    # Return whether total score meets the match threshold\n",
    "    return total_score >= match_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2086e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement article grouping and export results to a csv with the article_id and group_id of which it was grouped \n",
    "\n",
    "def group_articles(threshold: float = 0.7):\n",
    "    global df, cleaned_contents, kw_extractor, fair_ner_pipe\n",
    "    # Initialize article groups\n",
    "    article_groups = []\n",
    "    unclassified_articles = []  # To store articles with no entities/keywords\n",
    "    tqdm_bar = tqdm(enumerate(zip(cleaned_contents, df.iterrows())), \n",
    "                    desc=\"Grouping articles... (Groups: 0)\", \n",
    "                    total=len(cleaned_contents))\n",
    "\n",
    "    for i, (cleaned_content, (row_idx, row)) in tqdm_bar:\n",
    "        # Cleaned content into text\n",
    "        cleaned_content.insert(0, row.title)\n",
    "        cleaned_content = \" \".join(cleaned_content)\n",
    "        # Extract keywords\n",
    "        keywords = [keyword[0] for keyword in kw_extractor.extract_keywords(cleaned_content)]\n",
    "        # Extract entities\n",
    "        raw_entities = fair_ner_pipe(cleaned_content)\n",
    "        entities = combine_same_entities(_text=cleaned_content, _raw_entities=raw_entities)\n",
    "        entities = [Entity(raw_entity) for raw_entity in raw_entities]\n",
    "        \n",
    "        # Check if article has no entities and no keywords\n",
    "        if not entities or not keywords:\n",
    "            unclassified_articles.append(row['id'])\n",
    "            continue\n",
    "            \n",
    "        # Check if the article is relevant to any existing group\n",
    "        is_grouped = False\n",
    "        for group in article_groups:\n",
    "            if group.get_group_relevance_score(entities, keywords, match_threshold=threshold):\n",
    "                group.add_article(row['id'], entities, keywords)  # Use row['id'] instead of i\n",
    "                is_grouped = True\n",
    "                break\n",
    "        # If not grouped, create a new group\n",
    "        if not is_grouped:\n",
    "            new_group = ArticleGroup(identifier=len(article_groups))\n",
    "            new_group.add_article(row['id'], entities, keywords)  # Use row['id'] instead of i\n",
    "            article_groups.append(new_group)\n",
    "            # Update progress bar description with group count\n",
    "            tqdm_bar.set_description(f\"Grouping articles... (Threshold: {threshold}) (Groups: {len(article_groups)})\")\n",
    "\n",
    "    # Create an \"Unclassified\" group if there are any unclassified articles\n",
    "    if unclassified_articles:\n",
    "        unclassified_group = ArticleGroup(identifier=len(article_groups))\n",
    "        unclassified_group.name = \"Unclassified\"  # Set a name for the unclassified group\n",
    "        \n",
    "        # Add each unclassified article to this group\n",
    "        for article_idx in unclassified_articles:\n",
    "            # Adding with empty entities and keywords\n",
    "            unclassified_group.add_article(article_idx, [], [])\n",
    "        \n",
    "        # Add the unclassified group to the end of article_groups\n",
    "        article_groups.append(unclassified_group)\n",
    "        tqdm_bar.set_description(f\"Grouping articles... (Threshold: {threshold}) (Groups: {len(article_groups)})\")\n",
    "    \n",
    "    return article_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81842c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(article_groups, filename: str):\n",
    "    # Export the results to a csv\n",
    "    group_ids = []\n",
    "    for group in article_groups:\n",
    "        group_ids.extend([group.identifier] * len(group.get_group_article_ids()))\n",
    "\n",
    "    article_ids = []\n",
    "    for group in article_groups:\n",
    "        article_ids.extend(group.get_group_article_ids())\n",
    "\n",
    "\n",
    "    if len(article_ids) != len(group_ids):\n",
    "        raise ValueError(\"Mismatch between article_ids and group_ids lengths\")\n",
    "\n",
    "    results_df = pd.DataFrame({\"article_id\": article_ids, \"group_id\": group_ids})\n",
    "\n",
    "    results_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14fa532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping articles... (Threshold: 0.1) (Groups: 21): 100%|██████████| 354/354 [16:01<00:00,  2.72s/it]\n",
      "Grouping articles... (Threshold: 0.7) (Groups: 108): 100%|██████████| 354/354 [14:33<00:00,  2.47s/it]\n",
      "Grouping articles... (Threshold: 0.9) (Groups: 64):  29%|██▉       | 102/354 [03:12<07:56,  1.89s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[113]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m export_results(article_groups, \u001b[33m'\u001b[39m\u001b[33marticle_groups_0.7t.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 0.9 Threshold\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m article_groups = \u001b[43mgroup_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m export_results(article_groups, \u001b[33m'\u001b[39m\u001b[33marticle_groups_0.9t.csv\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[111]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mgroup_articles\u001b[39m\u001b[34m(threshold)\u001b[39m\n\u001b[32m     17\u001b[39m keywords = [keyword[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m kw_extractor.extract_keywords(cleaned_content)]\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Extract entities\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m raw_entities = \u001b[43mfair_ner_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m entities = combine_same_entities(_text=cleaned_content, _raw_entities=raw_entities)\n\u001b[32m     21\u001b[39m entities = [Entity(raw_entity) \u001b[38;5;28;01mfor\u001b[39;00m raw_entity \u001b[38;5;129;01min\u001b[39;00m raw_entities]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/transformers/pipelines/token_classification.py:250\u001b[39m, in \u001b[36mTokenClassificationPipeline.__call__\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m offset_mapping:\n\u001b[32m    248\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33moffset_mapping\u001b[39m\u001b[33m\"\u001b[39m] = offset_mapping\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1360\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[32m   1359\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m item = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:269\u001b[39m, in \u001b[36mPipelinePackIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    266\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    271\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch.Tensor):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1275\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1274\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/transformers/pipelines/token_classification.py:287\u001b[39m, in \u001b[36mTokenClassificationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs)\u001b[39m\n\u001b[32m    285\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.model(**model_inputs)[\u001b[32m0\u001b[39m]\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m     logits = output[\u001b[33m\"\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[32m0\u001b[39m]\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    291\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m\"\u001b[39m: logits,\n\u001b[32m    292\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mspecial_tokens_mask\u001b[39m\u001b[33m\"\u001b[39m: special_tokens_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    296\u001b[39m     **model_inputs,\n\u001b[32m    297\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:1525\u001b[39m, in \u001b[36mXLMRobertaForTokenClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1519\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1520\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1521\u001b[39m \u001b[33;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[32m   1522\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1523\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1525\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1526\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1531\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1532\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1533\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1535\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1537\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1539\u001b[39m sequence_output = \u001b[38;5;28mself\u001b[39m.dropout(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:977\u001b[39m, in \u001b[36mXLMRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    970\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    972\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    973\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    974\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    975\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m977\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    989\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    990\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:632\u001b[39m, in \u001b[36mXLMRobertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    621\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    622\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    623\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m         output_attentions,\n\u001b[32m    630\u001b[39m     )\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    642\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:563\u001b[39m, in \u001b[36mXLMRobertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    560\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    561\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    566\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    568\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/transformers/pytorch_utils.py:261\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    258\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:575\u001b[39m, in \u001b[36mXLMRobertaLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     intermediate_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n\u001b[32m    577\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:473\u001b[39m, in \u001b[36mXLMRobertaIntermediate.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.intermediate_act_fn(hidden_states)\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Trial of different thresholds and exporting results\n",
    "# 0.1 Threshold\n",
    "article_groups = group_articles(threshold=0.1)\n",
    "export_results(article_groups, 'article_groups_0.1t.csv')\n",
    "# 0.7 Threshold\n",
    "article_groups = group_articles(threshold=0.7)\n",
    "export_results(article_groups, 'article_groups_0.7t.csv')\n",
    "# 0.9 Threshold\n",
    "article_groups = group_articles(threshold=0.9)\n",
    "export_results(article_groups, 'article_groups_0.9t.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4687f7e",
   "metadata": {},
   "source": [
    "### Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3192b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation functions\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    normalized_mutual_info_score,\n",
    "    fowlkes_mallows_score,\n",
    "    rand_score,\n",
    "    homogeneity_completeness_v_measure\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f01e5f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = df[['id', 'event_id']].copy()\n",
    "gt_df.rename(columns={'id': 'article_id', 'event_id': 'gt_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3cf8e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import result csvs\n",
    "results_0_1 = pd.read_csv('results/article_groups_0.1t.csv')\n",
    "results_0_7 = pd.read_csv('results/article_groups_0.7t.csv')\n",
    "results_0_8 = pd.read_csv('results/article_groups_0.8t.csv')\n",
    "results_0_9 = pd.read_csv('results/article_groups_0.9t.csv')\n",
    "results_0_95 = pd.read_csv('results/article_groups_0.95t.csv')\n",
    "results_0_98 = pd.read_csv('results/article_groups_0.98t.csv')\n",
    "results_1 = pd.read_csv('results/article_groups_1t.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "83f55457",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_0_1 = results_0_1.merge(gt_df, on='article_id', how='left')\n",
    "results_0_7 = results_0_7.merge(gt_df, on='article_id', how='left')\n",
    "results_0_8 = results_0_8.merge(gt_df, on='article_id', how='left')\n",
    "results_0_9 = results_0_9.merge(gt_df, on='article_id', how='left')\n",
    "results_0_95 = results_0_95.merge(gt_df, on='article_id', how='left')\n",
    "results_0_98 = results_0_98.merge(gt_df, on='article_id', how='left')\n",
    "results_1 = results_1.merge(gt_df, on='article_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "acb5bad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.9</th>\n",
       "      <th>0.95</th>\n",
       "      <th>0.98</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rand Index</th>\n",
       "      <td>0.661705</td>\n",
       "      <td>0.773003</td>\n",
       "      <td>0.930427</td>\n",
       "      <td>0.973576</td>\n",
       "      <td>0.981226</td>\n",
       "      <td>0.979482</td>\n",
       "      <td>0.969847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adjusted Rand Index</th>\n",
       "      <td>0.022529</td>\n",
       "      <td>0.026626</td>\n",
       "      <td>0.099219</td>\n",
       "      <td>0.260240</td>\n",
       "      <td>0.334125</td>\n",
       "      <td>0.238809</td>\n",
       "      <td>0.211558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normalized Mutual Information</th>\n",
       "      <td>0.438296</td>\n",
       "      <td>0.631627</td>\n",
       "      <td>0.776134</td>\n",
       "      <td>0.857454</td>\n",
       "      <td>0.877957</td>\n",
       "      <td>0.865890</td>\n",
       "      <td>0.844180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fowlkes-Mallows Score</th>\n",
       "      <td>0.125061</td>\n",
       "      <td>0.112621</td>\n",
       "      <td>0.171474</td>\n",
       "      <td>0.301468</td>\n",
       "      <td>0.357833</td>\n",
       "      <td>0.256908</td>\n",
       "      <td>0.253860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0.1       0.7       0.8       0.9  \\\n",
       "Rand Index                     0.661705  0.773003  0.930427  0.973576   \n",
       "Adjusted Rand Index            0.022529  0.026626  0.099219  0.260240   \n",
       "Normalized Mutual Information  0.438296  0.631627  0.776134  0.857454   \n",
       "Fowlkes-Mallows Score          0.125061  0.112621  0.171474  0.301468   \n",
       "\n",
       "                                   0.95      0.98       1.0  \n",
       "Rand Index                     0.981226  0.979482  0.969847  \n",
       "Adjusted Rand Index            0.334125  0.238809  0.211558  \n",
       "Normalized Mutual Information  0.877957  0.865890  0.844180  \n",
       "Fowlkes-Mallows Score          0.357833  0.256908  0.253860  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate each result df and print the result as a single dataframe\n",
    "def evaluate_results(pred_labels: pd.Series, ground_truth: pd.Series) -> dict:\n",
    "    \"\"\"Evaluate the results using various clustering metrics.\n",
    "\n",
    "    Args:\n",
    "        pred_labels (pd.Series): Series containing the predicted group IDs.\n",
    "        ground_truth (pd.Series): Series containing the true group IDs for each article.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Use the provided predicted labels instead of a DataFrame column\n",
    "    predicted_labels = pred_labels.values\n",
    "    true_labels = ground_truth.values\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    metrics = {\n",
    "        \"Rand Index\": rand_score(true_labels, predicted_labels),\n",
    "        \"Adjusted Rand Index\": adjusted_rand_score(true_labels, predicted_labels),\n",
    "        \"Normalized Mutual Information\": normalized_mutual_info_score(true_labels, predicted_labels),\n",
    "        \"Fowlkes-Mallows Score\": fowlkes_mallows_score(true_labels, predicted_labels),\n",
    "        # \"Homogeneity\": homogeneity_completeness_v_measure(true_labels, predicted_labels)[0],\n",
    "        # \"Completeness\": homogeneity_completeness_v_measure(true_labels, predicted_labels)[1],\n",
    "        # \"V-Measure\": homogeneity_completeness_v_measure(true_labels, predicted_labels)[2]\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Evaluate each result df\n",
    "results_0_1_metrics = evaluate_results(results_0_1['group_id'], results_0_1['gt_id'])\n",
    "results_0_7_metrics = evaluate_results(results_0_7['group_id'], results_0_7['gt_id'])\n",
    "results_0_8_metrics = evaluate_results(results_0_8['group_id'], results_0_8['gt_id'])\n",
    "results_0_9_metrics = evaluate_results(results_0_9['group_id'], results_0_9['gt_id'])\n",
    "results_0_95_metrics = evaluate_results(results_0_95['group_id'], results_0_95['gt_id'])\n",
    "results_0_98_metrics = evaluate_results(results_0_98['group_id'], results_0_98['gt_id'])\n",
    "results_1_metrics = evaluate_results(results_1['group_id'], results_1['gt_id'])\n",
    "\n",
    "# Combine all metrics into a single dataframe\n",
    "result_vars = {\n",
    "    \"0.1\": results_0_1_metrics,\n",
    "    \"0.7\": results_0_7_metrics,\n",
    "    \"0.8\": results_0_8_metrics,\n",
    "    \"0.9\": results_0_9_metrics,\n",
    "    \"0.95\": results_0_95_metrics,\n",
    "    \"0.98\": results_0_98_metrics,\n",
    "    \"1.0\": results_1_metrics\n",
    "}\n",
    "results_df = pd.DataFrame(result_vars)\n",
    "# Print the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9719d5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Metric=Rand Index<br>Threshold=%{x}<br>Score=%{y}<extra></extra>",
         "legendgroup": "Rand Index",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Rand Index",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "0.1",
          "0.7",
          "0.8",
          "0.9",
          "0.95",
          "0.98",
          "1.0"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "SCTZS7As5T+9QMjFcLzoP4eVYYcOxu0/o+NayYgn7z9yrrCuNGbvP1rVGCPqV+8/ucP3OfwI7z8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Metric=Adjusted Rand Index<br>Threshold=%{x}<br>Score=%{y}<extra></extra>",
         "legendgroup": "Adjusted Rand Index",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Adjusted Rand Index",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "0.1",
          "0.7",
          "0.8",
          "0.9",
          "0.95",
          "0.98",
          "1.0"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "d9w9jrcRlz/0ZnY550ObP9UciTloZrk/GQzOdcWn0D/gzokUTmLVP1NwJHNLkc4/kbczNlMUyz8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Metric=Normalized Mutual Information<br>Threshold=%{x}<br>Score=%{y}<extra></extra>",
         "legendgroup": "Normalized Mutual Information",
         "line": {
          "color": "#00cc96",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Normalized Mutual Information",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "0.1",
          "0.7",
          "0.8",
          "0.9",
          "0.95",
          "0.98",
          "1.0"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "lxJQ8QgN3D9X37S/STbkP13YNIQW1ug/o7zpUkRw6z8xY649OBjsP0S/X+ldtes//kXEMIUD6z8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Metric=Fowlkes-Mallows Score<br>Threshold=%{x}<br>Score=%{y}<extra></extra>",
         "legendgroup": "Fowlkes-Mallows Score",
         "line": {
          "color": "#ab63fa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Fowlkes-Mallows Score",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "0.1",
          "0.7",
          "0.8",
          "0.9",
          "0.95",
          "0.98",
          "1.0"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "eLKK5/8BwD+4qy+sudS8P5uaVtHa8sU/uXqKIkFL0z9jF3SvvebWP9fuEGstcdA/gw05VD4/0D8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Metric"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Clustering Evaluation Metrics by Threshold"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Threshold"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "range": [
          0,
          1
         ],
         "title": {
          "text": "Score"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot with plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "plot_df = results_df.reset_index().melt(id_vars='index', var_name='Threshold', value_name='Score')\n",
    "\n",
    "# Create a line plot\n",
    "fig = px.line(plot_df, x='Threshold', y='Score', color='index', \n",
    "              title='Clustering Evaluation Metrics by Threshold',\n",
    "              labels={'index': 'Metric', 'Score': 'Score'},\n",
    "              range_y=[0, 1])\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
