{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Hbol",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Trial 2\n",
    "something different, maybe. Hope this works! :')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../df.pkl\", \"rb\") as f:\n",
    "#     df = pickle.load(f)\n",
    "df = pd.read_excel('../data/merged_set.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Tools testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lEQa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from yake import KeywordExtractor\n",
    "# from keybert import KeyBERT\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### YAKE (Looks good, but too many keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_extractor = KeywordExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.loc[df[\"event_id\"] == \"case_1\"].iloc[0][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = kw_extractor.extract_keywords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "RGSE",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('allowing Elon Musk', np.float64(0.0012443619293702544)),\n",
       " ('Elon Musk America', np.float64(0.0016008832896744504)),\n",
       " ('Musk America PAC', np.float64(0.005631785765392376)),\n",
       " ('Elon Musk', np.float64(0.006004674381945689)),\n",
       " ('allowing Elon', np.float64(0.015793797497147603)),\n",
       " ('Musk America', np.float64(0.020240416352859676)),\n",
       " ('Pennsylvania judge', np.float64(0.02226199325158571)),\n",
       " ('Judge Angelo Foglietta', np.float64(0.03145404645119408)),\n",
       " ('America PAC', np.float64(0.035297120716982114)),\n",
       " ('Pleas Court Judge', np.float64(0.05132581560866452)),\n",
       " ('Court Judge Angelo', np.float64(0.05132581560866452)),\n",
       " ('Common Pleas Court', np.float64(0.05474920997337077)),\n",
       " ('contest through Nov.', np.float64(0.05721328937878896)),\n",
       " ('ruling on Monday.', np.float64(0.05721328937878896)),\n",
       " ('America PAC attorney', np.float64(0.06127688564125262)),\n",
       " ('Nov.', np.float64(0.06536876963257338)),\n",
       " ('Monday.', np.float64(0.06536876963257338)),\n",
       " ('Pennsylvania', np.float64(0.0772582259727435)),\n",
       " ('Elon', np.float64(0.0772582259727435)),\n",
       " ('Musk', np.float64(0.0772582259727435))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "Kclp",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8351456024090593)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_sum = 0\n",
    "kw_sum = sum([keyword[1] for keyword in keywords])\n",
    "kw_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Keybert (Useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyBERT(model=\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nWHF",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('contest', 0.3704),\n",
       " ('pac', 0.3642),\n",
       " ('election', 0.363),\n",
       " ('trump', 0.3499),\n",
       " ('musk', 0.3343)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_kb = model.extract_keywords(text)\n",
    "keywords_kb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iLit",
   "metadata": {},
   "source": [
    "### NER_xlm-roberta-large-finetuned-conll03-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fair_ner_pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFacebookAI/xlm-roberta-large-finetuned-conll03-english\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:241\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    245\u001b[0m     )\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    247\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task\n",
      "\u001b[0;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "fair_ner_pipe = pipeline(\"token-classification\", model=\"FacebookAI/xlm-roberta-large-finetuned-conll03-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ROlb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_entities = fair_ner_pipe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "qnkX",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Pennsylvania judge is allowing Elon Musk\\'s America PAC to continue their $1 million a day contest through Nov. 5, according to a ruling on Monday.. . The ruling by Common Pleas Court Judge Angelo Foglietta comes after it was revealed during court proceedings that the contest\\'s winners are not randomly selected.. . \"The $1 million recipients are not chosen by chance,\" America PAC attorney Chris Gober said in court on Monday. \"We know exactly who will be announced as the $1 million recipient today and tomorrow.\". . NIKKI HALEY PENS SUPPORTIVE OP-ED IN FAVOR OF TRUMP AHEAD OF ELECTION DAY: \\'EASY CALL\\'. . CLICK HERE TO GET THE FOX NEWS APP. . The Associated Press contributed to this report.. . This is a breaking news situation. Check back with us for updates.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "TqIu",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-LOC',\n",
       "  'score': np.float32(0.99997306),\n",
       "  'index': 2,\n",
       "  'word': '▁Pennsylvania',\n",
       "  'start': 2,\n",
       "  'end': 14},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.9167749),\n",
       "  'index': 5,\n",
       "  'word': '▁',\n",
       "  'start': 24,\n",
       "  'end': 25},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.9999901),\n",
       "  'index': 7,\n",
       "  'word': '▁El',\n",
       "  'start': 33,\n",
       "  'end': 35},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.99999344),\n",
       "  'index': 8,\n",
       "  'word': 'on',\n",
       "  'start': 35,\n",
       "  'end': 37},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.9999771),\n",
       "  'index': 9,\n",
       "  'word': '▁Musk',\n",
       "  'start': 38,\n",
       "  'end': 42},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.9999908),\n",
       "  'index': 12,\n",
       "  'word': '▁America',\n",
       "  'start': 45,\n",
       "  'end': 52},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.7667245),\n",
       "  'index': 13,\n",
       "  'word': '▁',\n",
       "  'start': 53,\n",
       "  'end': 54},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.99995124),\n",
       "  'index': 14,\n",
       "  'word': 'PAC',\n",
       "  'start': 53,\n",
       "  'end': 56},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.9999951),\n",
       "  'index': 43,\n",
       "  'word': '▁Common',\n",
       "  'start': 166,\n",
       "  'end': 172},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.9999951),\n",
       "  'index': 44,\n",
       "  'word': '▁Ple',\n",
       "  'start': 173,\n",
       "  'end': 176},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.9999933),\n",
       "  'index': 45,\n",
       "  'word': 'as',\n",
       "  'start': 176,\n",
       "  'end': 178},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.9999957),\n",
       "  'index': 46,\n",
       "  'word': '▁Court',\n",
       "  'start': 179,\n",
       "  'end': 184},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.9999951),\n",
       "  'index': 49,\n",
       "  'word': '▁Angel',\n",
       "  'start': 191,\n",
       "  'end': 196},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.99999046),\n",
       "  'index': 50,\n",
       "  'word': 'o',\n",
       "  'start': 196,\n",
       "  'end': 197},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.9999883),\n",
       "  'index': 51,\n",
       "  'word': '▁Fog',\n",
       "  'start': 198,\n",
       "  'end': 201},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.9999833),\n",
       "  'index': 52,\n",
       "  'word': 'li',\n",
       "  'start': 201,\n",
       "  'end': 203},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.9999919),\n",
       "  'index': 53,\n",
       "  'word': 'etta',\n",
       "  'start': 203,\n",
       "  'end': 207},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.9999889),\n",
       "  'index': 94,\n",
       "  'word': '▁America',\n",
       "  'start': 372,\n",
       "  'end': 379},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.914708),\n",
       "  'index': 95,\n",
       "  'word': '▁',\n",
       "  'start': 380,\n",
       "  'end': 381},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.9999294),\n",
       "  'index': 96,\n",
       "  'word': 'PAC',\n",
       "  'start': 380,\n",
       "  'end': 383},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.99999595),\n",
       "  'index': 98,\n",
       "  'word': '▁Chris',\n",
       "  'start': 393,\n",
       "  'end': 398},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.99999344),\n",
       "  'index': 99,\n",
       "  'word': '▁Go',\n",
       "  'start': 399,\n",
       "  'end': 401},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.9999924),\n",
       "  'index': 100,\n",
       "  'word': 'ber',\n",
       "  'start': 401,\n",
       "  'end': 404},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.99995553),\n",
       "  'index': 127,\n",
       "  'word': '▁NI',\n",
       "  'start': 521,\n",
       "  'end': 523},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.9999882),\n",
       "  'index': 128,\n",
       "  'word': 'KKI',\n",
       "  'start': 523,\n",
       "  'end': 526},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.99999535),\n",
       "  'index': 129,\n",
       "  'word': '▁HA',\n",
       "  'start': 527,\n",
       "  'end': 529},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.9999907),\n",
       "  'index': 130,\n",
       "  'word': 'LEY',\n",
       "  'start': 529,\n",
       "  'end': 532},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.9999846),\n",
       "  'index': 143,\n",
       "  'word': '▁TRU',\n",
       "  'start': 567,\n",
       "  'end': 570},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.99998236),\n",
       "  'index': 144,\n",
       "  'word': 'MP',\n",
       "  'start': 570,\n",
       "  'end': 572},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.9196905),\n",
       "  'index': 167,\n",
       "  'word': '▁',\n",
       "  'start': 625,\n",
       "  'end': 626},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.9995433),\n",
       "  'index': 170,\n",
       "  'word': '▁F',\n",
       "  'start': 633,\n",
       "  'end': 634},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.9997609),\n",
       "  'index': 171,\n",
       "  'word': 'OX',\n",
       "  'start': 634,\n",
       "  'end': 636},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.9979122),\n",
       "  'index': 172,\n",
       "  'word': '▁NEWS',\n",
       "  'start': 637,\n",
       "  'end': 641},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.9999889),\n",
       "  'index': 178,\n",
       "  'word': '▁Associated',\n",
       "  'start': 653,\n",
       "  'end': 663},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.99996245),\n",
       "  'index': 179,\n",
       "  'word': '▁Press',\n",
       "  'start': 664,\n",
       "  'end': 669}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_same_entities(_text, _raw_entities):\n",
    "    prev_segment = None\n",
    "    entities = []\n",
    "    for segment in _raw_entities:\n",
    "        segment['word'] = segment['word'].replace('▁', ' ')\n",
    "        original_entity_word = segment['word']\n",
    "        entity_word = segment['word'].rstrip()\n",
    "        segment['end'] = segment['end'] - (len(original_entity_word) - len(entity_word))\n",
    "        if segment['word'].isspace():\n",
    "            continue\n",
    "        appended_to_prev_segment = False\n",
    "        if prev_segment is not None and prev_segment['entity'] == segment['entity']:\n",
    "            if prev_segment['end'] == segment['start']:\n",
    "                entities[-1]['word'] += segment['word']\n",
    "                appended_to_prev_segment = True\n",
    "            elif _text[prev_segment['end']:segment['start']].isspace():\n",
    "                entities[-1]['word'] += _text[prev_segment['end']:segment['start']] + segment['word']\n",
    "                appended_to_prev_segment = True\n",
    "\n",
    "            if appended_to_prev_segment:\n",
    "                entities[-1]['end'] = segment['end']\n",
    "                entities[-1]['score'] = (entities[-1]['score'] + segment['score'])/2\n",
    "\n",
    "        if not appended_to_prev_segment:\n",
    "            original_entity_word = entity_word\n",
    "            entity_word = entity_word.lstrip()\n",
    "            segment['start'] = segment['start'] + (len(original_entity_word) - len(entity_word))\n",
    "            entities.append({\n",
    "                'entity': segment['entity'],\n",
    "                'word': entity_word,\n",
    "                'score': segment['score'],\n",
    "                'start': segment['start'],\n",
    "                'end': segment['end']\n",
    "            })\n",
    "        entities[-1]['word'] = re.sub(r' +', ' ', entities[-1]['word']).strip()\n",
    "        prev_segment = segment.copy()\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "DnEU",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-LOC',\n",
       "  'word': 'Pennsylvania',\n",
       "  'score': np.float32(0.99997306),\n",
       "  'start': 3,\n",
       "  'end': 14},\n",
       " {'entity': 'I-PER',\n",
       "  'word': 'Elon Musk',\n",
       "  'score': np.float32(0.99998444),\n",
       "  'start': 34,\n",
       "  'end': 42},\n",
       " {'entity': 'I-ORG',\n",
       "  'word': 'America PAC',\n",
       "  'score': np.float32(0.99997103),\n",
       "  'start': 46,\n",
       "  'end': 56},\n",
       " {'entity': 'I-ORG',\n",
       "  'word': 'Common Pleas Court',\n",
       "  'score': np.float32(0.999995),\n",
       "  'start': 167,\n",
       "  'end': 184},\n",
       " {'entity': 'I-PER',\n",
       "  'word': 'Angelo Foglietta',\n",
       "  'score': np.float32(0.9999894),\n",
       "  'start': 192,\n",
       "  'end': 207},\n",
       " {'entity': 'I-ORG',\n",
       "  'word': 'America PAC',\n",
       "  'score': np.float32(0.9999592),\n",
       "  'start': 373,\n",
       "  'end': 383},\n",
       " {'entity': 'I-PER',\n",
       "  'word': 'Chris Gober',\n",
       "  'score': np.float32(0.99999356),\n",
       "  'start': 394,\n",
       "  'end': 404},\n",
       " {'entity': 'I-PER',\n",
       "  'word': 'NIKKI HALEY',\n",
       "  'score': np.float32(0.9999871),\n",
       "  'start': 522,\n",
       "  'end': 532},\n",
       " {'entity': 'I-PER',\n",
       "  'word': 'TRUMP',\n",
       "  'score': np.float32(0.9999835),\n",
       "  'start': 568,\n",
       "  'end': 572},\n",
       " {'entity': 'I-ORG',\n",
       "  'word': 'FOX NEWS',\n",
       "  'score': np.float32(0.99878216),\n",
       "  'start': 634,\n",
       "  'end': 641},\n",
       " {'entity': 'I-ORG',\n",
       "  'word': 'Associated Press',\n",
       "  'score': np.float32(0.9999757),\n",
       "  'start': 654,\n",
       "  'end': 669}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_same_entities(text, raw_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ulZA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecfG",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.trial_2.text_filteration import ContentFilterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_filter = ContentFilterer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ZBYS",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df = df.iloc[:200]\n",
    "selected_df = selected_df._append(df.loc[df['event_id'] == 'case_1'])\n",
    "selected_df = selected_df._append(df.loc[df['event_id'] == 'case_2'])\n",
    "selected_df = selected_df._append(df.loc[df['event_id'] == 'case_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aLJB",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>images</th>\n",
       "      <th>event_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>a49cb3a4-6fb3-4bb0-b8b8-4631c85d7669</td>\n",
       "      <td>IIT Roorkee team finds superbug’s defense mech...</td>\n",
       "      <td>['Suraj Maurya', 'Margin-Bottom Important', '....</td>\n",
       "      <td>NaT</td>\n",
       "      <td>The Press United | International News Analysis...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Facebook Twitter Reddit WhatsApp LinkedIn Pint...</td>\n",
       "      <td>['Health', 'Education', 'India', 'Entertainmen...</td>\n",
       "      <td>https://thepressunited.com/wp-content/uploads/...</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>5747aab6-275a-4986-8d28-2a291f241b07</td>\n",
       "      <td>17p spice could stop 'superbugs' that can't be...</td>\n",
       "      <td>['Andrew Nuttall', 'Image', 'Getty', 'Newsbyan...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Surrey Live</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Something went wrong, please try again later.\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i2-prod.getsurrey.co.uk/news/health/ar...</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>3ac6af53-b337-4a2a-88c5-92fe3d45c0cb</td>\n",
       "      <td>IIT Roorkee team finds superbug’s defense mech...</td>\n",
       "      <td>['Newsroom Network', 'Newsroom Odisha Network']</td>\n",
       "      <td>NaT</td>\n",
       "      <td>News Room Odisha</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Roorkee: Scientists at the Indian Institute of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.newsroomodisha.com/wp-content/uplo...</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>eafa01c6-3078-49bf-913f-71697594fe1b</td>\n",
       "      <td>Heidi Klum unveils Halloween costume as ET wit...</td>\n",
       "      <td>['Stephanie Giang-Paunon Larry Fink', 'Stephan...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>FOX News</td>\n",
       "      <td>https://www.foxnews.com/entertainment/heidi-kl...</td>\n",
       "      <td>The queen of Halloween, Heidi Klum, was \"out o...</td>\n",
       "      <td>['#HeidiHalloween']</td>\n",
       "      <td>['https://a57.foxnews.com/static.foxnews.com/f...</td>\n",
       "      <td>case_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>6e353ebb-749d-4520-be04-650305979a44</td>\n",
       "      <td>Heidi Klum Dresses as E.T. for Her Annual Hall...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>TMZ</td>\n",
       "      <td>https://www.tmz.com/2024/11/01/heidi-klum-et-c...</td>\n",
       "      <td>Heidi Klum's Halloween bash is always a wild e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['https://imagez.tmz.com/image/92/16by9/2024/1...</td>\n",
       "      <td>case_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id  \\\n",
       "197  a49cb3a4-6fb3-4bb0-b8b8-4631c85d7669   \n",
       "198  5747aab6-275a-4986-8d28-2a291f241b07   \n",
       "199  3ac6af53-b337-4a2a-88c5-92fe3d45c0cb   \n",
       "76   eafa01c6-3078-49bf-913f-71697594fe1b   \n",
       "77   6e353ebb-749d-4520-be04-650305979a44   \n",
       "\n",
       "                                                 title  \\\n",
       "197  IIT Roorkee team finds superbug’s defense mech...   \n",
       "198  17p spice could stop 'superbugs' that can't be...   \n",
       "199  IIT Roorkee team finds superbug’s defense mech...   \n",
       "76   Heidi Klum unveils Halloween costume as ET wit...   \n",
       "77   Heidi Klum Dresses as E.T. for Her Annual Hall...   \n",
       "\n",
       "                                                author publication_date  \\\n",
       "197  ['Suraj Maurya', 'Margin-Bottom Important', '....              NaT   \n",
       "198  ['Andrew Nuttall', 'Image', 'Getty', 'Newsbyan...              NaT   \n",
       "199    ['Newsroom Network', 'Newsroom Odisha Network']              NaT   \n",
       "76   ['Stephanie Giang-Paunon Larry Fink', 'Stephan...              NaT   \n",
       "77                                                 NaN              NaT   \n",
       "\n",
       "                                                source  \\\n",
       "197  The Press United | International News Analysis...   \n",
       "198                                        Surrey Live   \n",
       "199                                   News Room Odisha   \n",
       "76                                            FOX News   \n",
       "77                                                 TMZ   \n",
       "\n",
       "                                                   url  \\\n",
       "197                                                NaN   \n",
       "198                                                NaN   \n",
       "199                                                NaN   \n",
       "76   https://www.foxnews.com/entertainment/heidi-kl...   \n",
       "77   https://www.tmz.com/2024/11/01/heidi-klum-et-c...   \n",
       "\n",
       "                                               content  \\\n",
       "197  Facebook Twitter Reddit WhatsApp LinkedIn Pint...   \n",
       "198  Something went wrong, please try again later.\\...   \n",
       "199  Roorkee: Scientists at the Indian Institute of...   \n",
       "76   The queen of Halloween, Heidi Klum, was \"out o...   \n",
       "77   Heidi Klum's Halloween bash is always a wild e...   \n",
       "\n",
       "                                                  tags  \\\n",
       "197  ['Health', 'Education', 'India', 'Entertainmen...   \n",
       "198                                                NaN   \n",
       "199                                                NaN   \n",
       "76                                 ['#HeidiHalloween']   \n",
       "77                                                 NaN   \n",
       "\n",
       "                                                images event_id  \n",
       "197  https://thepressunited.com/wp-content/uploads/...      134  \n",
       "198  https://i2-prod.getsurrey.co.uk/news/health/ar...      134  \n",
       "199  https://www.newsroomodisha.com/wp-content/uplo...      134  \n",
       "76   ['https://a57.foxnews.com/static.foxnews.com/f...   case_3  \n",
       "77   ['https://imagez.tmz.com/image/92/16by9/2024/1...   case_3  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "nHfw",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering contents with CCF...:  16%|█▌        | 32/202 [00:26<02:21,  1.21it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# For cleaning all the contents\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# for idx, row in tqdm(df.iterrows(), desc=\"Filtering contents with CCF...\", total=len(df)):\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     cleaned_content = content_filter.filter_text(row['title'], row['content'])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     cleaned_contents.append(cleaned_content)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# For cleaning of selected sample set\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m tqdm(selected_df\u001b[38;5;241m.\u001b[39miterrows(), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltering contents with CCF...\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(selected_df)):\n\u001b[0;32m---> 10\u001b[0m     cleaned_content \u001b[38;5;241m=\u001b[39m \u001b[43mcontent_filter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     cleaned_contents\u001b[38;5;241m.\u001b[39mappend(cleaned_content)\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/analysis/2_consolidation/scripts/trial_2/text_filteration.py:222\u001b[0m, in \u001b[0;36mContentFilterer.filter_text\u001b[0;34m(self, title, content_text)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfilter_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, title: \u001b[38;5;28mstr\u001b[39m, content_text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    221\u001b[0m     text_blocks \u001b[38;5;241m=\u001b[39m text_block_splitting(content_text)\n\u001b[0;32m--> 222\u001b[0m     filtered_text_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mccf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_blocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filtered_text_blocks \u001b[38;5;28;01mif\u001b[39;00m filtered_text_blocks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/analysis/2_consolidation/scripts/trial_2/text_filteration.py:118\u001b[0m, in \u001b[0;36mCCF.filter_text\u001b[0;34m(self, reference_text, victim_corpus, similarity_threshold, search_batch_size)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03mFilter the content using the CCF model.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m \n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     corpus \u001b[38;5;241m=\u001b[39m [reference_text] \u001b[38;5;241m+\u001b[39m victim_corpus\n\u001b[0;32m--> 118\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_embeddings_for_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     reference_text_embedding \u001b[38;5;241m=\u001b[39m embeddings[\u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m    120\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m embeddings[\u001b[38;5;241m1\u001b[39m:, :]\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/analysis/2_consolidation/scripts/trial_2/text_filteration.py:22\u001b[0m, in \u001b[0;36mEmbeddingModel.generate_embeddings_for_corpus\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_embeddings_for_corpus\u001b[39m(\u001b[38;5;28mself\u001b[39m, corpus):\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Generate normalized embeddings for a list of strings.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m   embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m   normalized_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([emb \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(emb) \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m embeddings], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m normalized_embeddings\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:623\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 623\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    625\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:690\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m    689\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/sentence_transformers/models/Transformer.py:442\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    437\u001b[0m     key: value\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    440\u001b[0m }\n\u001b[0;32m--> 442\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/transformers/pytorch_utils.py:255\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/Data Volume/Projects/Storion/Codebase/storion_aggregation_analysis/.venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cleaned_contents = []\n",
    "\n",
    "# For cleaning all the contents\n",
    "# for idx, row in tqdm(df.iterrows(), desc=\"Filtering contents with CCF...\", total=len(df)):\n",
    "#     cleaned_content = content_filter.filter_text(row['title'], row['content'])\n",
    "#     cleaned_contents.append(cleaned_content)\n",
    "\n",
    "# For cleaning of selected sample set\n",
    "for idx, row in tqdm(selected_df.iterrows(), desc=\"Filtering contents with CCF...\", total=len(selected_df)):\n",
    "    cleaned_content = content_filter.filter_text(row['title'], row['content'])\n",
    "    cleaned_contents.append(cleaned_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83dc8b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"cleaned_contents.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cleaned_contents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8ba6f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"cleaned_contents.pkl\", \"rb\") as f:\n",
    "    cleaned_contents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "AjVT",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When will mail-in and absentee ballots be counted?  . One such hold-up is the work it takes to process and count mail-in ballots  . Because each state has different rules regarding the timeline for processing and counting mail-in ballots, many election officials cannot start tabulating these early ballots until Election Day or even after the polls close that night  Make sure you are aware of when your state starts processing and counting mail-in ballots  Local laws limit when election officials can process ballots  Absentee and mail-in ballots must be processed before they can be counted  . The process of handling mailed ballots varies by state, according to the National Conference of State Legislatures. Typically, it includes several steps: checking the ballot envelope, verifying that the signature on the return envelope matches the voter's signature on file, opening the envelope, and preparing the ballot for counting. In most states, officials cannot feed the ballot into the tabulator until the polls close on Election Day  . Absentee and mail ballots have to be processed before they can be tallied  . Forty-three states and the Virgin Islands allow election officials to begin processing these early ballots before Election Day  . allow election officials to begin processing these early ballots before Election Day. In Connecticut and Ohio, election officials can choose to start processing early ballots before at their discretion  . can choose to start processing early ballots before at their discretion. In seven states—Alabama, Mississippi, New Hampshire, Pennsylvania, South Dakota, West Virginia and Wisconsin, as well as Washington, D.C.,—mailed-in ballots can be processed on Election Day and before the polls close  . Mail ballots typically require more time and resources to process than in-person votes, especially when there is a large volume, as was the case in the 2020 election. The limited time available for processing ballots on Election Day can exacerbate delays. This issue became particularly pronounced during the pandemic when laws were modified to allow more people to vote early, resulting in significant challenges for election workers handling absentee and mail ballots  . Most states begin the tabulation of mail ballots on Election Day, although there are some important differences among them. In certain states, it is illegal to share results from mail-in ballots before the polls close, which typically occurs between 6 p.m. ET and 12 a.m. ET  . The 23 states that mandate counting begin on Election Day before polls close  . The 12 states that allow processing and counting to start before Election Day:  . Connecticut allows local registrars of voters to determine when to start counting ballots, according to the NCSL. In the Virgin Islands, counting begins after absentee ballots have been processed, though the exact timing is not specified. Puerto Rico does not indicate when counting can begin.\n",
      "[('Election Day', np.float64(0.004425217979375226)), ('election', np.float64(0.011729159096357644)), ('election officials', np.float64(0.01634784488346899)), ('ballots', np.float64(0.017868367489607042)), ('Day', np.float64(0.02267705922427515)), ('mail-in ballots', np.float64(0.03163296094223978)), ('early ballots', np.float64(0.035551844734587015)), ('processing early ballots', np.float64(0.04611586727162785)), ('mail ballots', np.float64(0.0462206946319196)), ('processing', np.float64(0.050252133813572165)), ('polls close', np.float64(0.05222812768770787)), ('counting mail-in ballots', np.float64(0.05684921144424314)), ('mail-in', np.float64(0.06065743834302691)), ('start processing early', np.float64(0.06172814173778644)), ('absentee', np.float64(0.06401043213691253)), ('states', np.float64(0.0646215357748597)), ('counting', np.float64(0.06473820596156803)), ('early', np.float64(0.06703787044775292)), ('officials', np.float64(0.06710730910378497)), ('absentee ballots', np.float64(0.0682431639587318))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-MISC',\n",
       "  'word': 'Election Day',\n",
       "  'score': np.float32(0.99986774),\n",
       "  'start': 314,\n",
       "  'end': 325},\n",
       " {'entity': 'I-ORG',\n",
       "  'word': 'National Conference of State Legislatures',\n",
       "  'score': np.float32(0.99933076),\n",
       "  'start': 672,\n",
       "  'end': 712},\n",
       " {'entity': 'I-MISC',\n",
       "  'word': 'Election Day',\n",
       "  'score': np.float32(0.99974537),\n",
       "  'start': 1028,\n",
       "  'end': 1039},\n",
       " {'entity': 'I-LOC',\n",
       "  'word': 'Virgin Island',\n",
       "  'score': np.float32(0.9838215),\n",
       "  'start': 1148,\n",
       "  'end': 1160},\n",
       " {'entity': 'I-MISC',\n",
       "  'word': 'Election Day',\n",
       "  'score': np.float32(0.9997926),\n",
       "  'start': 1235,\n",
       "  'end': 1246},\n",
       " {'entity': 'I-MISC',\n",
       "  'word': 'Election Day',\n",
       "  'score': np.float32(0.99977684),\n",
       "  'start': 1323,\n",
       "  'end': 1334},\n",
       " {'entity': 'I-LOC',\n",
       "  'word': 'Connecticut',\n",
       "  'score': np.float32(0.9998577),\n",
       "  'start': 1340,\n",
       "  'end': 1350},\n",
       " {'entity': 'I-LOC',\n",
       "  'word': 'Ohio',\n",
       "  'score': np.float32(0.99997365),\n",
       "  'start': 1356,\n",
       "  'end': 1359},\n",
       " {'entity': 'I-LOC',\n",
       "  'word': 'Alabama',\n",
       "  'score': np.float32(0.9999711),\n",
       "  'start': 1544,\n",
       "  'end': 1551},\n",
       " {'entity': 'I-LOC',\n",
       "  'word': 'Mississippi',\n",
       "  'score': np.float32(0.9999893),\n",
       "  'start': 1554,\n",
       "  'end': 1564},\n",
       " {'entity': 'I-LOC',\n",
       "  'word': 'New Hampshire',\n",
       "  'score': np.float32(0.9997778),\n",
       "  'start': 1567,\n",
       "  'end': 1579},\n",
       " {'entity': 'I-LOC',\n",
       "  'word': 'Pennsylvania',\n",
       "  'score': np.float32(0.99998736),\n",
       "  'start': 1582,\n",
       "  'end': 1593},\n",
       " {'entity': 'I-LOC',\n",
       "  'word': 'South Dakota',\n",
       "  'score': np.float32(0.99999),\n",
       "  'start': 1596,\n",
       "  'end': 1607},\n",
       " {'entity': 'I-LOC',\n",
       "  'word': 'West Virginia',\n",
       "  'score': np.float32(0.99998903),\n",
       "  'start': 1610,\n",
       "  'end': 1622},\n",
       " {'entity': 'I-LOC',\n",
       "  'word': 'Wisconsin',\n",
       "  'score': np.float32(0.9999914),\n",
       "  'start': 1628,\n",
       "  'end': 1636},\n",
       " {'entity': 'I-LOC',\n",
       "  'word': 'Washington, D.C.',\n",
       "  'score': np.float32(0.9997503),\n",
       "  'start': 1650,\n",
       "  'end': 1665},\n",
       " {'entity': 'I-MISC',\n",
       "  'word': 'Election Day',\n",
       "  'score': np.float32(0.999661),\n",
       "  'start': 1706,\n",
       "  'end': 1717},\n",
       " {'entity': 'I-MISC',\n",
       "  'word': 'Election Day',\n",
       "  'score': np.float32(0.9997833),\n",
       "  'start': 1969,\n",
       "  'end': 1980},\n",
       " {'entity': 'I-MISC',\n",
       "  'word': 'Election Day',\n",
       "  'score': np.float32(0.99972713),\n",
       "  'start': 2274,\n",
       "  'end': 2285}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample of keyword extraction & NER\n",
    "_sample_text = \" \".join(cleaned_contents[19])\n",
    "print(_sample_text)\n",
    "sample_keywords = kw_extractor.extract_keywords(_sample_text)\n",
    "print(sample_keywords)\n",
    "sample_entities = combine_same_entities(_sample_text, fair_ner_pipe(_sample_text))\n",
    "sample_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pHFh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_non_entity_keywords(_keywords, _entities):\n",
    "    entity_list = set([entity['word'] for entity in _entities])\n",
    "    _keywords = [keyword for keyword in _keywords if keyword[0] not in entity_list]\n",
    "    return _keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "NCOB",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to calculate similarity between two strings\n",
    "def calculate_similarity(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "# Function to create a similarity matrix for all words\n",
    "def create_similarity_matrix(primary_word_list, secondary_word_list):\n",
    "    height = len(primary_word_list)\n",
    "    width = len(secondary_word_list)\n",
    "    matrix = [[0] * width for _ in range(height)]\n",
    "\n",
    "    for i, word1 in enumerate(primary_word_list):\n",
    "        for j, word2 in enumerate(secondary_word_list):\n",
    "            if i != j:  # Avoid self-comparison\n",
    "                matrix[i][j] = calculate_similarity(word1, word2)\n",
    "    return matrix\n",
    "\n",
    "# Function to group words based on similarity matrix\n",
    "def group_similar_words(word_list, top_values_threshold=None):\n",
    "    similarity_matrix = np.array(create_similarity_matrix(word_list, word_list))\n",
    "    # Dynamic thresholding \n",
    "    top_n_values = len(similarity_matrix[similarity_matrix>top_values_threshold]) if top_values_threshold is not None else similarity_matrix.shape[0]//1\n",
    "    top_n_values = 1 if top_n_values==0 else top_n_values\n",
    "    top_n_values *= -1\n",
    "    top_n_similarities = np.partition(similarity_matrix.flatten(), top_n_values)[top_n_values:]\n",
    "    threshold = np.mean(top_n_similarities)\n",
    "    print(f\"Threshold set to {threshold}\")\n",
    "    groups = []\n",
    "    visited = set()\n",
    "\n",
    "    for i, word in enumerate(word_list):\n",
    "        if word not in visited:\n",
    "            group = set()\n",
    "            for j, similarity in enumerate(similarity_matrix[i]):\n",
    "                if similarity >= threshold:\n",
    "                    group.add(word_list[j])\n",
    "            group.add(word)  # Include the current word\n",
    "            groups.append(list(group))\n",
    "            visited.update(group)  # Mark all words in this group as visited\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aqbW",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold set to 0.4924780791305558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Election Day', 'election', 'election officials'],\n",
       " ['mail-in ballots',\n",
       "  'absentee ballots',\n",
       "  'early ballots',\n",
       "  'ballots',\n",
       "  'mail ballots'],\n",
       " ['Day', 'early'],\n",
       " ['start processing early',\n",
       "  'mail-in ballots',\n",
       "  'absentee ballots',\n",
       "  'processing',\n",
       "  'counting mail-in ballots',\n",
       "  'early ballots',\n",
       "  'mail ballots',\n",
       "  'processing early ballots'],\n",
       " ['polls close'],\n",
       " ['mail-in ballots', 'mail-in'],\n",
       " ['absentee ballots', 'absentee'],\n",
       " ['states'],\n",
       " ['election', 'counting', 'counting mail-in ballots'],\n",
       " ['officials', 'election officials']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_kw_groups = group_similar_words([keyword[0] for keyword in sample_keywords], top_values_threshold=0.3)\n",
    "sample_kw_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e79b5603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SequenceMatcher(None, sample_kw_groups[0][1], sample_entities[2]['word']).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcff1256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "election\n",
      "Election Day\n"
     ]
    }
   ],
   "source": [
    "print(sample_kw_groups[0][1])\n",
    "print(sample_entities[2]['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42690fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_same_entities(entities, threshold=0.55):\n",
    "    entity_word_list = [entity['word'] for entity in entities]\n",
    "    entity_similarities = np.array(create_similarity_matrix(entity_word_list, entity_word_list))\n",
    "    entity_groups = []\n",
    "    group_id = 0\n",
    "    entity_group_ids = [None] * len(entities)\n",
    "    is_grouped = [False] * len(entities)\n",
    "    for i, entity in enumerate(entities):\n",
    "        if not is_grouped[i]:\n",
    "            group = [entity]\n",
    "            is_grouped[i] = True\n",
    "            entity_group_ids[i] = group_id\n",
    "            shorlisted_similarities = (entity_similarities[i, :] > threshold)\n",
    "            for j, is_shorlisted in enumerate(shorlisted_similarities):\n",
    "                if not is_shorlisted or is_grouped[j]:\n",
    "                    continue\n",
    "                if entity['entity'] == entities[j]['entity']:\n",
    "                    group.append(entities[j])\n",
    "                    is_grouped[j] = True\n",
    "                    entity_group_ids[j] = group_id\n",
    "            entity_groups.append(group)\n",
    "            group_id += 1\n",
    "    return entity_groups, entity_group_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fda8c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'entity': 'I-MISC',\n",
       "   'word': 'Election Day',\n",
       "   'score': np.float32(0.99986774),\n",
       "   'start': 314,\n",
       "   'end': 325},\n",
       "  {'entity': 'I-MISC',\n",
       "   'word': 'Election Day',\n",
       "   'score': np.float32(0.99974537),\n",
       "   'start': 1028,\n",
       "   'end': 1039},\n",
       "  {'entity': 'I-MISC',\n",
       "   'word': 'Election Day',\n",
       "   'score': np.float32(0.9997926),\n",
       "   'start': 1235,\n",
       "   'end': 1246},\n",
       "  {'entity': 'I-MISC',\n",
       "   'word': 'Election Day',\n",
       "   'score': np.float32(0.99977684),\n",
       "   'start': 1323,\n",
       "   'end': 1334},\n",
       "  {'entity': 'I-MISC',\n",
       "   'word': 'Election Day',\n",
       "   'score': np.float32(0.999661),\n",
       "   'start': 1706,\n",
       "   'end': 1717},\n",
       "  {'entity': 'I-MISC',\n",
       "   'word': 'Election Day',\n",
       "   'score': np.float32(0.9997833),\n",
       "   'start': 1969,\n",
       "   'end': 1980},\n",
       "  {'entity': 'I-MISC',\n",
       "   'word': 'Election Day',\n",
       "   'score': np.float32(0.99972713),\n",
       "   'start': 2274,\n",
       "   'end': 2285}],\n",
       " [{'entity': 'I-ORG',\n",
       "   'word': 'National Conference of State Legislatures',\n",
       "   'score': np.float32(0.99933076),\n",
       "   'start': 672,\n",
       "   'end': 712}],\n",
       " [{'entity': 'I-LOC',\n",
       "   'word': 'Virgin Island',\n",
       "   'score': np.float32(0.9838215),\n",
       "   'start': 1148,\n",
       "   'end': 1160}],\n",
       " [{'entity': 'I-LOC',\n",
       "   'word': 'Connecticut',\n",
       "   'score': np.float32(0.9998577),\n",
       "   'start': 1340,\n",
       "   'end': 1350}],\n",
       " [{'entity': 'I-LOC',\n",
       "   'word': 'Ohio',\n",
       "   'score': np.float32(0.99997365),\n",
       "   'start': 1356,\n",
       "   'end': 1359}],\n",
       " [{'entity': 'I-LOC',\n",
       "   'word': 'Alabama',\n",
       "   'score': np.float32(0.9999711),\n",
       "   'start': 1544,\n",
       "   'end': 1551}],\n",
       " [{'entity': 'I-LOC',\n",
       "   'word': 'Mississippi',\n",
       "   'score': np.float32(0.9999893),\n",
       "   'start': 1554,\n",
       "   'end': 1564}],\n",
       " [{'entity': 'I-LOC',\n",
       "   'word': 'New Hampshire',\n",
       "   'score': np.float32(0.9997778),\n",
       "   'start': 1567,\n",
       "   'end': 1579}],\n",
       " [{'entity': 'I-LOC',\n",
       "   'word': 'Pennsylvania',\n",
       "   'score': np.float32(0.99998736),\n",
       "   'start': 1582,\n",
       "   'end': 1593}],\n",
       " [{'entity': 'I-LOC',\n",
       "   'word': 'South Dakota',\n",
       "   'score': np.float32(0.99999),\n",
       "   'start': 1596,\n",
       "   'end': 1607}],\n",
       " [{'entity': 'I-LOC',\n",
       "   'word': 'West Virginia',\n",
       "   'score': np.float32(0.99998903),\n",
       "   'start': 1610,\n",
       "   'end': 1622}],\n",
       " [{'entity': 'I-LOC',\n",
       "   'word': 'Wisconsin',\n",
       "   'score': np.float32(0.9999914),\n",
       "   'start': 1628,\n",
       "   'end': 1636}],\n",
       " [{'entity': 'I-LOC',\n",
       "   'word': 'Washington, D.C.',\n",
       "   'score': np.float32(0.9997503),\n",
       "   'start': 1650,\n",
       "   'end': 1665}]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_entity_groups, sample_entity_group_ids = identify_same_entities(entities=sample_entities, threshold=0.55)\n",
    "sample_entity_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9703e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Entity:\n",
    "    name: str\n",
    "    entity_type: str\n",
    "    score: np.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6388102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_entity_relevant_keyword_groups(keyword_groups, entity_groups, entity_group_ids):\n",
    "  entity_keyword_group_ids = dict() # Initialize dictionary to store keyword group ids for each entity group\n",
    "  max_keyword_group_length = max([len(keyword_group) for keyword_group in keyword_groups]) # Get maximum number of keywords in a group\n",
    "  keyword_lengths = np.array([[len(keyword) for keyword in keyword_group] + ([-1]*(max_keyword_group_length-len(keyword_group))) for keyword_group in keyword_groups]) # Get lengths of all keywords in each group\n",
    "  for group_id, entity_group in enumerate(entity_groups): # Iterate over each entity group\n",
    "    entity_keyword_group_ids[group_id] = [] # Initialize list to store keyword group ids for the entity group\n",
    "    entity_words = [entity['word'] for entity in entity_group] # Get all words in the entity group\n",
    "    entity_word_lengths = [len(entity['word']) for entity in entity_group] # Get lengths of all words in the entity group\n",
    "    compared_entity_word = entity_words[np.argmax(entity_word_lengths)] # Get the longest word in the entity group\n",
    "    compared_entity_word_length = len(compared_entity_word) # Get the length of the longest word in the entity group\n",
    "    # Threshold need to be non-linearly inversed propotional (the more length of the word, the less propotion of the length of the word as threshold, the threshold = length x propotion, the propotion should be between 0 and 1)\n",
    "    default_proportion = 0.7 # For 3 words\n",
    "    acceptable_keyword_length_threshold = compared_entity_word_length * (default_proportion / math.log(compared_entity_word_length + 1, 10)) # Non-linearly inversed propotional threshold\n",
    "    acceptable_keyword_length_threshold = 1 if acceptable_keyword_length_threshold < 1 else acceptable_keyword_length_threshold # Minimum threshold to 1 (in case of very short words)\n",
    "    acceptable_keyword_length = [compared_entity_word_length - acceptable_keyword_length_threshold, compared_entity_word_length + acceptable_keyword_length_threshold] # Acceptable length range for keywords with the threshold\n",
    "    shortlisted_keyword_groups = [any(group_checks) for group_checks in np.bitwise_and(keyword_lengths >= acceptable_keyword_length[0], keyword_lengths <= acceptable_keyword_length[1])] # Shortlist groups with atleast one keyword in the acceptable length range\n",
    "    for i, is_shortlisted in enumerate(shortlisted_keyword_groups): # Iterate over shortlisted keyword groups\n",
    "        # Exit if the keyword group is not shortlisted\n",
    "        if not is_shortlisted:\n",
    "            continue\n",
    "        # Check if the entity group is relevant to the keyword group\n",
    "        similarity_scores = [SequenceMatcher(None, entity_word, keyword).ratio() for entity_word in entity_words for keyword in keyword_groups[i]]\n",
    "        if max(similarity_scores) == 1 or np.mean(similarity_scores) > 0.8:\n",
    "            entity_keyword_group_ids[group_id].append(i)\n",
    "    entity_keyword_group_ids[group_id] = None if len(entity_keyword_group_ids[group_id]) == 0 else entity_keyword_group_ids[group_id] # Set to None if no keyword group is relevant to the entity group\n",
    "  return entity_keyword_group_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dc00bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Election Day': [['Election Day', 'election', 'election officials']],\n",
       " 'National Conference of State Legislatures': None,\n",
       " 'Virgin Island': None,\n",
       " 'Connecticut': None,\n",
       " 'Ohio': None,\n",
       " 'Alabama': None,\n",
       " 'Mississippi': None,\n",
       " 'New Hampshire': None,\n",
       " 'Pennsylvania': None,\n",
       " 'South Dakota': None,\n",
       " 'West Virginia': None,\n",
       " 'Wisconsin': None,\n",
       " 'Washington, D.C.': None}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_keyword_group_links = link_entity_relevant_keyword_groups(sample_kw_groups, sample_entity_groups, sample_entity_group_ids)\n",
    "sample_entity_group_word_list = [[entity['word'] for entity in entity_group] for entity_group in sample_entity_groups]\n",
    "{ max(sample_entity_group_word_list[entity_group_id], key=len): None if relevent_kw_group_ids is None else [sample_kw_groups[kw_group_id] for kw_group_id in relevent_kw_group_ids] for entity_group_id, relevent_kw_group_ids in entity_keyword_group_links.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e7c3d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Election Day',\n",
       "  'Election Day',\n",
       "  'Election Day',\n",
       "  'Election Day',\n",
       "  'Election Day',\n",
       "  'Election Day',\n",
       "  'Election Day'],\n",
       " ['National Conference of State Legislatures'],\n",
       " ['Virgin Island'],\n",
       " ['Connecticut'],\n",
       " ['Ohio'],\n",
       " ['Alabama'],\n",
       " ['Mississippi'],\n",
       " ['New Hampshire'],\n",
       " ['Pennsylvania'],\n",
       " ['South Dakota'],\n",
       " ['West Virginia'],\n",
       " ['Wisconsin'],\n",
       " ['Washington, D.C.']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_entity_group_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b525cb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Election Day'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(sample_entity_group_word_list[0], key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08901048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Election Day', 'election', 'election officials'],\n",
       " ['mail-in ballots',\n",
       "  'absentee ballots',\n",
       "  'early ballots',\n",
       "  'ballots',\n",
       "  'mail ballots'],\n",
       " ['Day', 'early'],\n",
       " ['start processing early',\n",
       "  'mail-in ballots',\n",
       "  'absentee ballots',\n",
       "  'processing',\n",
       "  'counting mail-in ballots',\n",
       "  'early ballots',\n",
       "  'mail ballots',\n",
       "  'processing early ballots'],\n",
       " ['polls close'],\n",
       " ['mail-in ballots', 'mail-in'],\n",
       " ['absentee ballots', 'absentee'],\n",
       " ['states'],\n",
       " ['election', 'counting', 'counting mail-in ballots'],\n",
       " ['officials', 'election officials']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_kw_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dfdae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the moment flatten the ner and keyword groups by length\n",
    "proceeding_entity_list = [max(entity_group, key=len) for entity_group in sample_entity_group_word_list]\n",
    "proceeding_keyword_list = [max(keyword_group, key=len) for keyword_group in sample_kw_groups]\n",
    "proceeding_keyword_list = list(set(proceeding_keyword_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c23e4769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['states',\n",
       " 'mail-in ballots',\n",
       " 'election officials',\n",
       " 'early',\n",
       " 'absentee ballots',\n",
       " 'counting mail-in ballots',\n",
       " 'polls close']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proceeding_keyword_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f09f46",
   "metadata": {},
   "source": [
    "### Article Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20e80485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "507ee515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/33.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m^C\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93414209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spacy model\n",
    "spacy_model = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de93ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_similarity_matrix(primary_entity_list, secondary_entity_list):\n",
    "  name_similarity_matrix = np.array(create_similarity_matrix(\n",
    "    [entity.word for entity in primary_entity_list], \n",
    "    [entity.word for entity in secondary_entity_list]\n",
    "    ))\n",
    "  type_match_matrix = np.array([[1 if entity.entity_type == entity2.entity_type else 0 for entity2 in secondary_entity_list] for entity in primary_entity_list])\n",
    "  assert name_similarity_matrix.shape == type_match_matrix.shape # Ensure the shape of both matrices are the same\n",
    "  final_score_matrix = name_similarity_matrix * type_match_matrix\n",
    "  return final_score_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a752d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_weights = {\n",
    "  \"Person\": 0.4,\n",
    "  \"Location\": 0.5,\n",
    "  \"Organization\": 0.3,\n",
    "  \"Miscellaneous\": 0.2\n",
    "}\n",
    "\n",
    "class ArticleGroup:\n",
    "  identifier: int\n",
    "  __internal_entities: list[str]\n",
    "  __internal_keywords: list[str]\n",
    "  __group_article_ids: list[str]\n",
    "  __total_entity_scorable: float\n",
    "\n",
    "  def __init__(self, identifier: int = -1):\n",
    "    self.identifier = identifier\n",
    "    self.__internal_entities = []\n",
    "    self.__total_entity_scorable = 0\n",
    "    self.__internal_keywords = []\n",
    "\n",
    "  def get_group_article_ids(self):\n",
    "    return self.__group_article_ids\n",
    "  \n",
    "  def get_group_keywords(self):\n",
    "    return self.__internal_keywords\n",
    "  \n",
    "  def get_group_entities(self):\n",
    "    return self.__internal_entities\n",
    "  \n",
    "  def add_article_id(self, article_id: str, entities: list[str], keywords: list[str]):\n",
    "    global entity_weights\n",
    "    self.__group_article_ids.append(article_id)\n",
    "    ## Total entity scorable calculation\n",
    "    self.__total_entity_scorable = sum([entity_weights[entity.entity_type] for entity in entities])\n",
    "    pass\n",
    "\n",
    "  def get_group_relevance_score(self, entities: list[str], keywords: list[str], entity_match_threshold: float = 0.8, keyword_match_threshold: float = 0.8) -> float:\n",
    "    global entity_weights\n",
    "    # Check if the entities are relevant to the group (make sure the article is about the same subject and people) (40% weight)\n",
    "    ## Get entity similarity score matrix\n",
    "    entity_similarity_matrix = get_entity_similarity_matrix(self.__internal_entities, entities)\n",
    "    ## Create entity type matrix\n",
    "    no_of_article_entities = len(entities)\n",
    "    entity_type_matrix = []\n",
    "    for i, entity in enumerate(self.__internal_entities):\n",
    "      max_index = np.argmax(entity_similarity_matrix[i])\n",
    "      if entity_similarity_matrix[i, max_index] < entity_match_threshold:\n",
    "        entity_type_matrix.append([None] * no_of_article_entities)\n",
    "      else:\n",
    "        entity_type_matrix.append([None] * max_index + [entities[max_index].entity_type] + [None] * (no_of_article_entities - max_index - 1))\n",
    "    entity_type_matrix = np.array(entity_type_matrix)\n",
    "    # entity_type_matrix = np.array([[entity.entity_type if entity_similarity_matrix[i][j] > entity_match_threshold else None for j in range(no_of_article_entities)] for i, entity in enumerate(self.__internal_entities)])\n",
    "    ## Calculate the entity weight matrix\n",
    "    entity_weight_matrix = np.array([[entity_weights[entity_type] if entity_type is not None else 0 for entity_type in entity_type_matrix_row] for entity_type_matrix_row in entity_type_matrix])\n",
    "    ## Calculate the entity score\n",
    "    entity_score = np.sum(entity_weight_matrix)\n",
    "    ## Normalize the entity score to the 40% weight\n",
    "    normalized_entity_score = (entity_score / self.__total_entity_scorable) * 0.4\n",
    "    if normalized_entity_score == 0: return 0.0\n",
    "    # Check if the keywords are relevant to the group (make sure the article is talking about the same event within the subject) (60% weight)\n",
    "    ## Get keyword similarity matrix\n",
    "    kw_similarity_matrix = np.array(create_similarity_matrix(self.__internal_keywords, keywords))\n",
    "    ## Calculate the keyword score\n",
    "    normalized_keyword_score = (np.sum(kw_similarity_matrix) / len(keywords)) * 0.6 if keywords else 0.0\n",
    "    # Return the total score\n",
    "    return normalized_entity_score + normalized_keyword_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
